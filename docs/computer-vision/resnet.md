# Deep Residual Learning for Image Recognition (ResNet)

<div class="paper-meta" markdown>

**ä½œè€…**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
**æœºæ„**: Microsoft Research
**ä¼šè®®**: CVPR 2016
**è®ºæ–‡é“¾æ¥**: [arXiv:1512.03385](https://arxiv.org/abs/1512.03385)

</div>

<div class="paper-tags" markdown>
<span class="paper-tag">ResNet</span>
<span class="paper-tag">æ®‹å·®è¿æ¥</span>
<span class="paper-tag">å›¾åƒåˆ†ç±»</span>
<span class="paper-tag">æ·±åº¦ç½‘ç»œ</span>
</div>

## ğŸ“ æ‘˜è¦

æœ¬æ–‡æå‡ºäº†æ·±åº¦æ®‹å·®å­¦ä¹ æ¡†æ¶ï¼ˆResNetï¼‰ï¼Œé€šè¿‡å¼•å…¥æ®‹å·®è¿æ¥ï¼ˆskip connectionï¼‰è§£å†³äº†æ·±åº¦ç¥ç»ç½‘ç»œçš„é€€åŒ–é—®é¢˜ã€‚ResNet åœ¨ ImageNet åˆ†ç±»ã€COCO æ£€æµ‹å’Œåˆ†å‰²ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹¶èµ¢å¾—äº† ILSVRC 2015 å›¾åƒåˆ†ç±»å† å†›ã€‚

**ä¸»è¦è´¡çŒ®**ï¼š
- æå‡ºæ®‹å·®å­¦ä¹ æ¡†æ¶ï¼Œä½¿è®­ç»ƒè¶…è¿‡ 100 å±‚çš„ç½‘ç»œæˆä¸ºå¯èƒ½
- åœ¨ ImageNet ä¸Šä»¥ 152 å±‚ç½‘ç»œè¾¾åˆ° 3.57% top-5 é”™è¯¯ç‡
- è¯æ˜äº†ææ·±ç½‘ç»œçš„æœ‰æ•ˆæ€§

## ğŸ’¡ æ ¸å¿ƒæ€æƒ³

### é€€åŒ–é—®é¢˜ï¼ˆDegradation Problemï¼‰

å®éªŒå‘ç°ï¼Œç®€å•åœ°å †å æ›´å¤šå±‚ä¼šå¯¼è‡´**é€€åŒ–**ï¼šæ›´æ·±çš„ç½‘ç»œè®­ç»ƒè¯¯å·®åè€Œæ›´é«˜ï¼ˆä¸æ˜¯è¿‡æ‹Ÿåˆï¼‰ã€‚

### æ®‹å·®å­¦ä¹ ï¼ˆResidual Learningï¼‰

ä¸å…¶å­¦ä¹ ç›´æ¥æ˜ å°„ $H(x)$ï¼Œä¸å¦‚å­¦ä¹ æ®‹å·®æ˜ å°„ï¼š

$$
F(x) = H(x) - x
$$

åŸå§‹æ˜ å°„å˜ä¸ºï¼š

$$
H(x) = F(x) + x
$$

**ç›´è§‰**ï¼šå­¦ä¹ "å¦‚ä½•è°ƒæ•´"æ¯”å­¦ä¹ "å®Œæ•´è¾“å‡º"æ›´å®¹æ˜“ã€‚

### æ®‹å·®å—ï¼ˆResidual Blockï¼‰

```
x â†’ [Conv-BN-ReLU-Conv-BN] â†’ F(x)
 â†“                              â†“
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ (+) â†’ ReLU â†’ output
           identity
```

## ğŸ—ï¸ ç½‘ç»œæ¶æ„

### ResNet-50/101/152

| å±‚å | è¾“å‡ºå°ºå¯¸ | ResNet-50 | ResNet-101 | ResNet-152 |
|------|----------|-----------|------------|------------|
| conv1 | 112Ã—112 | 7Ã—7, 64, stride 2 | 7Ã—7, 64, stride 2 | 7Ã—7, 64, stride 2 |
| conv2_x | 56Ã—56 | 3Ã—3 max pool, stride 2<br>[1Ã—1,64<br>3Ã—3,64<br>1Ã—1,256] Ã—3 | Ã—3 | Ã—3 |
| conv3_x | 28Ã—28 | [1Ã—1,128<br>3Ã—3,128<br>1Ã—1,512] Ã—4 | Ã—4 | Ã—8 |
| conv4_x | 14Ã—14 | [1Ã—1,256<br>3Ã—3,256<br>1Ã—1,1024] Ã—6 | Ã—23 | Ã—36 |
| conv5_x | 7Ã—7 | [1Ã—1,512<br>3Ã—3,512<br>1Ã—1,2048] Ã—3 | Ã—3 | Ã—3 |
| | 1Ã—1 | average pool, 1000-d fc, softmax | | |

### ç“¶é¢ˆç»“æ„ï¼ˆBottleneckï¼‰

ä¸ºäº†é™ä½è®¡ç®—é‡ï¼ŒResNet-50/101/152 ä½¿ç”¨ç“¶é¢ˆè®¾è®¡ï¼š

```
1Ã—1 conv (é™ç»´) â†’ 3Ã—3 conv â†’ 1Ã—1 conv (å‡ç»´)
```

## ğŸ“Š å®éªŒç»“æœ

### ImageNet åˆ†ç±»

| æ¨¡å‹ | Top-1 Error | Top-5 Error | å‚æ•°é‡ |
|------|-------------|-------------|--------|
| VGG-16 | - | 7.3% | 138M |
| GoogleNet | - | 6.7% | - |
| **ResNet-34** | 24.5% | - | 21.8M |
| **ResNet-50** | 22.9% | - | 25.6M |
| **ResNet-101** | 21.8% | - | 44.5M |
| **ResNet-152** | **21.4%** | **3.57%** | 60.2M |

### COCO ç›®æ ‡æ£€æµ‹

åŸºäº Faster R-CNN + ResNet-101ï¼š

- **mAP**: æå‡ 6% ç›¸æ¯” VGG-16

### æ¶ˆèå®éªŒ

**æ’ç­‰æ˜ å°„ vs æŠ•å½±**ï¼š
- æ’ç­‰ shortcut æ•ˆæœæœ€å¥½
- æŠ•å½± shortcut ç•¥æœ‰æå‡ä½†å¢åŠ å‚æ•°

**ç½‘ç»œæ·±åº¦**ï¼š
- 18å±‚ â†’ 34å±‚ï¼šé”™è¯¯ç‡æ˜¾è‘—ä¸‹é™
- 50å±‚ â†’ 101å±‚ â†’ 152å±‚ï¼šæŒç»­æ”¹è¿›

## ğŸ¤” ä¸ªäººç¬”è®°

### ä¸ºä»€ä¹ˆæ®‹å·®è¿æ¥æœ‰æ•ˆï¼Ÿ

1. **æ¢¯åº¦æµåŠ¨**ï¼šshortcut æä¾›äº†æ¢¯åº¦çš„ç›´æ¥é€šè·¯ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±
2. **é›†æˆæ•ˆåº”**ï¼šç½‘ç»œå¯ä»¥çœ‹ä½œå¤šä¸ªæµ…å±‚ç½‘ç»œçš„é›†æˆ
3. **ä¼˜åŒ–æ™¯è§‚**ï¼šæ®‹å·®å­¦ä¹ ä½¿æŸå¤±å‡½æ•°æ›´å¹³æ»‘ï¼Œæ›´å®¹æ˜“ä¼˜åŒ–

### è®¾è®¡è¦ç‚¹

- **æ‰¹å½’ä¸€åŒ–ï¼ˆBNï¼‰**ï¼šæ¯ä¸ªå·ç§¯åéƒ½ä½¿ç”¨ BN
- **æ—  Dropout**ï¼šResNet ä¸ä½¿ç”¨ dropout
- **æ•°æ®å¢å¼º**ï¼šæ ‡å‡†çš„è£å‰ªã€ç¿»è½¬ã€é¢œè‰²æŠ–åŠ¨

### åç»­å‘å±•

- **ResNeXt**ï¼šå¼•å…¥åˆ†ç»„å·ç§¯ï¼Œæå‡è¡¨ç¤ºèƒ½åŠ›
- **DenseNet**ï¼šå¯†é›†è¿æ¥ï¼Œè¿›ä¸€æ­¥åŠ å¼ºç‰¹å¾å¤ç”¨
- **EfficientNet**ï¼šé€šè¿‡ NAS è‡ªåŠ¨æœç´¢é«˜æ•ˆæ¶æ„
- **ResNet å˜ä½“**ï¼šPre-activation ResNetã€Wide ResNet ç­‰

### å½±å“

ResNet çš„æ®‹å·®è¿æ¥æ€æƒ³å½±å“æ·±è¿œï¼š

- **Transformer**ï¼šä¹Ÿä½¿ç”¨æ®‹å·®è¿æ¥
- **ç›®æ ‡æ£€æµ‹**ï¼šFPNã€Mask R-CNN ç­‰éƒ½åŸºäº ResNet
- **å·¥ä¸šåº”ç”¨**ï¼šResNet æˆä¸ºè®¡ç®—æœºè§†è§‰çš„æ ‡å‡†éª¨å¹²ç½‘ç»œ

## ğŸ”— ç›¸å…³è®ºæ–‡

- **VGG**: Very Deep Convolutional Networks - æ›´æ—©çš„æ·±åº¦ç½‘ç»œå°è¯•
- **Inception/GoogleNet**: Going Deeper with Convolutions - å¤šå°ºåº¦å·ç§¯
- **ResNeXt**: Aggregated Residual Transformations - ResNet æ”¹è¿›
- **DenseNet**: Densely Connected Convolutional Networks - å¯†é›†è¿æ¥

## ğŸ“Œ å…³é”®ä»£ç ç‰‡æ®µ

```python
import torch
import torch.nn as nn

class BasicBlock(nn.Module):
    """ResNet-18/34 çš„åŸºæœ¬æ®‹å·®å—"""
    expansion = 1

    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = torch.relu(out)
        return out


class Bottleneck(nn.Module):
    """ResNet-50/101/152 çš„ç“¶é¢ˆæ®‹å·®å—"""
    expansion = 4

    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.conv3 = nn.Conv2d(out_channels, out_channels * 4, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels * 4)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels * 4:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels * 4, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels * 4)
            )

    def forward(self, x):
        out = torch.relu(self.bn1(self.conv1(x)))
        out = torch.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = torch.relu(out)
        return out
```

---

*é˜…è¯»æ—¥æœŸï¼š2024å¹´*
*ç¬”è®°çŠ¶æ€ï¼šå·²å®Œæˆ*
