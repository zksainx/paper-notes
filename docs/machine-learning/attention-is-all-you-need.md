# Attention Is All You Need

<div class="paper-meta" markdown>

**ä½œè€…**: Ashish Vaswani, Noam Shazeer, Niki Parmar, et al.
**æœºæ„**: Google Brain, Google Research
**ä¼šè®®**: NeurIPS 2017
**è®ºæ–‡é“¾æ¥**: [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

</div>

<div class="paper-tags" markdown>
<span class="paper-tag">Transformer</span>
<span class="paper-tag">æ³¨æ„åŠ›æœºåˆ¶</span>
<span class="paper-tag">åºåˆ—å»ºæ¨¡</span>
</div>

## ğŸ“ æ‘˜è¦

æœ¬æ–‡æå‡ºäº† Transformer æ¶æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åºåˆ—è½¬æ¢æ¨¡å‹ï¼Œæ‘’å¼ƒäº†å¾ªç¯å’Œå·ç§¯ç»“æ„ã€‚åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šï¼ŒTransformer ä¸ä»…å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ï¼Œè€Œä¸”è®­ç»ƒé€Ÿåº¦æ˜¾è‘—æå‡ã€‚

**ä¸»è¦è´¡çŒ®**ï¼š
- æå‡ºäº†çº¯æ³¨æ„åŠ›æ¶æ„ï¼Œæ— éœ€å¾ªç¯æˆ–å·ç§¯
- å¼•å…¥å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰æœºåˆ¶
- åœ¨ WMT 2014 è‹±å¾·å’Œè‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸Šè¾¾åˆ° SOTA

## ğŸ’¡ æ ¸å¿ƒæ€æƒ³

### 1. è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰

é€šè¿‡è®¡ç®—åºåˆ—ä¸­æ¯ä¸ªä½ç½®ä¸å…¶ä»–æ‰€æœ‰ä½ç½®çš„å…³è”åº¦ï¼Œæ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

å…¶ä¸­ $Q$ï¼ˆQueryï¼‰ã€$K$ï¼ˆKeyï¼‰ã€$V$ï¼ˆValueï¼‰æ˜¯è¾“å…¥çš„çº¿æ€§å˜æ¢ã€‚

### 2. å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

å¹¶è¡Œä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´ï¼š

$$
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,...,\text{head}_h)W^O
$$

### 3. ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

ç”±äºæ¨¡å‹æ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œéœ€è¦é¢å¤–æ·»åŠ ä½ç½®ä¿¡æ¯ï¼š

$$
\begin{align}
PE_{(pos, 2i)} &= \sin(pos / 10000^{2i/d_{model}}) \\
PE_{(pos, 2i+1)} &= \cos(pos / 10000^{2i/d_{model}})
\end{align}
$$

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

Transformer é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼š

**ç¼–ç å™¨**ï¼š
- 6 å±‚å †å 
- æ¯å±‚åŒ…å«ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ
- æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–

**è§£ç å™¨**ï¼š
- 6 å±‚å †å 
- æ¯å±‚åŒ…å«ï¼šæ©ç å¤šå¤´è‡ªæ³¨æ„åŠ› + ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ
- æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–

## ğŸ“Š å®éªŒç»“æœ

### æœºå™¨ç¿»è¯‘æ€§èƒ½

| æ¨¡å‹ | WMT'14 EN-DE | WMT'14 EN-FR |
|------|--------------|--------------|
| **Transformer (big)** | **28.4 BLEU** | **41.8 BLEU** |
| ConvS2S | 25.2 BLEU | 40.5 BLEU |

### è®­ç»ƒæ•ˆç‡

- **è®­ç»ƒæ—¶é—´**ï¼šåœ¨ 8 ä¸ª P100 GPU ä¸Šè®­ç»ƒ 3.5 å¤©ï¼ˆbase æ¨¡å‹ï¼‰
- **å¹¶è¡ŒåŒ–**ï¼šç›¸æ¯” RNN å¯ä»¥æ›´å¥½åœ°å¹¶è¡ŒåŒ–ï¼Œè®­ç»ƒé€Ÿåº¦å¿«æ•°å€

## ğŸ¤” ä¸ªäººç¬”è®°

### ä¼˜åŠ¿

1. **å¹¶è¡ŒåŒ–èƒ½åŠ›å¼º**ï¼šä¸åƒ RNN éœ€è¦é¡ºåºå¤„ç†ï¼Œå¯ä»¥å¹¶è¡Œè®¡ç®—æ‰€æœ‰ä½ç½®
2. **é•¿è·ç¦»ä¾èµ–**ï¼šé€šè¿‡è‡ªæ³¨æ„åŠ›ç›´æ¥å»ºæ¨¡ä»»æ„è·ç¦»çš„ä¾èµ–å…³ç³»
3. **å¯è§£é‡Šæ€§**ï¼šæ³¨æ„åŠ›æƒé‡å¯ä»¥å¯è§†åŒ–ï¼Œäº†è§£æ¨¡å‹å…³æ³¨å“ªäº›éƒ¨åˆ†

### å±€é™æ€§

1. **è®¡ç®—å¤æ‚åº¦**ï¼šè‡ªæ³¨æ„åŠ›çš„å¤æ‚åº¦æ˜¯ $O(n^2)$ï¼Œåºåˆ—è¶Šé•¿è®¡ç®—é‡è¶Šå¤§
2. **ä½ç½®ç¼–ç **ï¼šå›ºå®šçš„æ­£å¼¦ä½ç½®ç¼–ç å¯èƒ½ä¸æ˜¯æœ€ä¼˜é€‰æ‹©
3. **å°æ•°æ®é›†**ï¼šåœ¨æ•°æ®é‡è¾ƒå°æ—¶ï¼ŒTransformer å¯èƒ½ä¸å¦‚ RNN

### å½±å“ä¸å¯å‘

- **NLP é©å‘½**ï¼šTransformer æˆä¸º BERTã€GPT ç­‰é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€
- **è·¨é¢†åŸŸåº”ç”¨**ï¼šVision Transformer (ViT) å°†å…¶æˆåŠŸåº”ç”¨äºè®¡ç®—æœºè§†è§‰
- **ç ”ç©¶æ–¹å‘**ï¼šå¦‚ä½•é™ä½è‡ªæ³¨æ„åŠ›çš„å¤æ‚åº¦æˆä¸ºé‡è¦ç ”ç©¶æ–¹å‘ï¼ˆLinformerã€Performer ç­‰ï¼‰

## ğŸ”— ç›¸å…³è®ºæ–‡

- [BERT: Pre-training of Deep Bidirectional Transformers](../nlp/bert.md) - åŸºäº Transformer çš„é¢„è®­ç»ƒæ¨¡å‹
- **GPT Series** - ç”Ÿæˆå¼é¢„è®­ç»ƒ Transformer
- **Vision Transformer (ViT)** - Transformer åœ¨è§†è§‰é¢†åŸŸçš„åº”ç”¨

## ğŸ“Œ å…³é”®ä»£ç ç‰‡æ®µ

```python
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # Linear projections and reshape
        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention, V)

        # Concatenate heads and apply final linear
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        return self.W_o(output)
```

---

*é˜…è¯»æ—¥æœŸï¼š2024å¹´*
*ç¬”è®°çŠ¶æ€ï¼šå·²å®Œæˆ*
