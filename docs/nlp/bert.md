# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

<div class="paper-meta" markdown>

**ä½œè€…**: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
**æœºæ„**: Google AI Language
**ä¼šè®®**: NAACL 2019
**è®ºæ–‡é“¾æ¥**: [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)

</div>

<div class="paper-tags" markdown>
<span class="paper-tag">BERT</span>
<span class="paper-tag">é¢„è®­ç»ƒ</span>
<span class="paper-tag">åŒå‘Transformer</span>
<span class="paper-tag">è¯­è¨€æ¨¡å‹</span>
</div>

## ğŸ“ æ‘˜è¦

BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯ä¸€ä¸ªåŸºäº Transformer çš„åŒå‘é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚é€šè¿‡åœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒï¼ŒBERT åœ¨ 11 ä¸ª NLP ä»»åŠ¡ä¸Šåˆ·æ–°äº† SOTA è®°å½•ã€‚

**ä¸»è¦è´¡çŒ®**ï¼š
- æå‡ºæ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰å®ç°çœŸæ­£çš„åŒå‘é¢„è®­ç»ƒ
- å¼•å…¥ä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNSPï¼‰ä»»åŠ¡å­¦ä¹ å¥å­å…³ç³»
- åœ¨å¤šä¸ª NLP ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—æå‡

## ğŸ’¡ æ ¸å¿ƒæ€æƒ³

### 1. åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡

ä¸ä¼ ç»Ÿçš„å•å‘è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰ä¸åŒï¼ŒBERT é€šè¿‡æ©ç è¯­è¨€æ¨¡å‹åŒæ—¶åˆ©ç”¨å·¦å³ä¸Šä¸‹æ–‡ï¼š

```
ä¼ ç»Ÿ LM:  The cat [sat] on the mat
           â†â†â†â†â†
GPT:      The cat [sat] on the mat
           â†’â†’â†’â†’â†’
BERT:     The cat [MASK] on the mat
           â†â†â†â†’â†’â†’â†’
```

### 2. æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMasked Language Modelï¼‰

éšæœºæ©ç›–è¾“å…¥ä¸­ 15% çš„è¯ï¼Œè®­ç»ƒæ¨¡å‹é¢„æµ‹è¢«æ©ç›–çš„è¯ï¼š

- 80% æ›¿æ¢ä¸º `[MASK]`
- 10% æ›¿æ¢ä¸ºéšæœºè¯
- 10% ä¿æŒä¸å˜

**ç›®æ ‡**ï¼šæœ€å¤§åŒ–è¢«æ©ç›–è¯çš„æ¡ä»¶æ¦‚ç‡

### 3. ä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNext Sentence Predictionï¼‰

ç»™å®šå¥å­å¯¹ (A, B)ï¼Œé¢„æµ‹ B æ˜¯å¦æ˜¯ A çš„ä¸‹ä¸€å¥ï¼š

```
Input:  [CLS] Sentence A [SEP] Sentence B [SEP]
Label:  IsNext / NotNext
```

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

BERT åŸºäº Transformer ç¼–ç å™¨ï¼Œæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼š

| æ¨¡å‹ | å±‚æ•° | éšè—ç»´åº¦ | æ³¨æ„åŠ›å¤´ | å‚æ•°é‡ |
|------|------|----------|----------|--------|
| BERT-Base | 12 | 768 | 12 | 110M |
| BERT-Large | 24 | 1024 | 16 | 340M |

### è¾“å…¥è¡¨ç¤º

ä¸‰ç§åµŒå…¥æ±‚å’Œï¼š

```
Token Embeddings:    [CLS] my dog is cute [SEP] he likes play ##ing [SEP]
Segment Embeddings:  E_A   E_A E_A E_A E_A E_A  E_B  E_B   E_B  E_B   E_B
Position Embeddings: E_0   E_1 E_2 E_3 E_4 E_5  E_6  E_7   E_8  E_9   E_10
```

### ç‰¹æ®Šæ ‡è®°

- `[CLS]`ï¼šåºåˆ—åˆ†ç±»æ ‡è®°ï¼Œå…¶è¾“å‡ºç”¨äºåˆ†ç±»ä»»åŠ¡
- `[SEP]`ï¼šå¥å­åˆ†éš”ç¬¦
- `[MASK]`ï¼šæ©ç æ ‡è®°

## ğŸ“Š å®éªŒç»“æœ

### GLUE åŸºå‡†æµ‹è¯•

| ä»»åŠ¡ | æŒ‡æ ‡ | BERT-Base | BERT-Large | ä¹‹å‰ SOTA |
|------|------|-----------|------------|-----------|
| MNLI | Acc | 84.6/83.4 | **86.7/85.9** | 80.5/80.1 |
| QQP | F1 | 71.2 | **72.1** | 66.1 |
| QNLI | Acc | 90.5 | **92.7** | 87.4 |
| SST-2 | Acc | 93.5 | **94.9** | 93.2 |
| CoLA | Matthews Corr | 52.1 | **60.5** | 35.0 |

### SQuAD é—®ç­”

| æ¨¡å‹ | SQuAD 1.1 F1 | SQuAD 2.0 F1 |
|------|--------------|--------------|
| BERT-Base | 88.5 | 76.3 |
| **BERT-Large** | **93.2** | **83.1** |
| äººç±»è¡¨ç° | 91.2 | 86.8 |

BERT-Large åœ¨ SQuAD 1.1 ä¸Šè¶…è¶Šäººç±»è¡¨ç°ï¼

### å‘½åå®ä½“è¯†åˆ«ï¼ˆCoNLL-2003 NERï¼‰

| æ¨¡å‹ | F1 |
|------|----|
| ä¹‹å‰ SOTA | 92.6 |
| **BERT-Large** | **92.8** |

## ğŸ¤” ä¸ªäººç¬”è®°

### å…³é”®è®¾è®¡

1. **WordPiece åˆ†è¯**ï¼šå¤„ç†æœªç™»å½•è¯ï¼Œå‡å°è¯è¡¨å¤§å°
2. **Segment Embeddings**ï¼šåŒºåˆ†å¥å­å¯¹ä¸­çš„ä¸åŒå¥å­
3. **é¢„è®­ç»ƒ + å¾®è°ƒèŒƒå¼**ï¼šé€šç”¨é¢„è®­ç»ƒ â†’ ä»»åŠ¡ç‰¹å®šå¾®è°ƒ

### ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ

- **å¤§è§„æ¨¡é¢„è®­ç»ƒ**ï¼šä» BooksCorpus (800M è¯) + Wikipedia (2,500M è¯) å­¦ä¹ é€šç”¨è¯­è¨€è¡¨ç¤º
- **åŒå‘å»ºæ¨¡**ï¼šæ¯”å•å‘æ¨¡å‹èƒ½æ•è·æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡
- **æ·±åº¦ Transformer**ï¼šå¼ºå¤§çš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›

### å±€é™æ€§

1. **è®¡ç®—æˆæœ¬é«˜**ï¼šé¢„è®­ç»ƒéœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼ˆ4-16 TPUï¼Œå‡ å¤©æ—¶é—´ï¼‰
2. **NSP ä»»åŠ¡äº‰è®®**ï¼šåç»­ç ”ç©¶ï¼ˆRoBERTaï¼‰è¡¨æ˜ NSP å¯èƒ½ä¸å¿…è¦
3. **[MASK] ä¸ä¸€è‡´**ï¼šé¢„è®­ç»ƒç”¨ [MASK]ï¼Œä½†å¾®è°ƒæ—¶æ²¡æœ‰
4. **æœ€å¤§é•¿åº¦é™åˆ¶**ï¼š512 tokens é™åˆ¶äº†é•¿æ–‡æœ¬å¤„ç†

### åç»­å‘å±•

- **RoBERTa**ï¼šç§»é™¤ NSPï¼ŒåŠ¨æ€æ©ç ï¼Œæ›´å¤§æ‰¹æ¬¡
- **ALBERT**ï¼šå‚æ•°å…±äº«ï¼Œå¥å­é¡ºåºé¢„æµ‹ï¼ˆSOPï¼‰
- **ELECTRA**ï¼šåˆ¤åˆ«å¼é¢„è®­ç»ƒï¼Œæ›´é«˜æ•ˆ
- **T5**ï¼šç»Ÿä¸€ text-to-text æ¡†æ¶
- **GPT-3/ChatGPT**ï¼šå¤§è§„æ¨¡ç”Ÿæˆå¼é¢„è®­ç»ƒ

### åº”ç”¨æŠ€å·§

**ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ**ï¼š
- åˆ†ç±»ä»»åŠ¡ï¼šä½¿ç”¨ [CLS] è¾“å‡º + çº¿æ€§å±‚
- åºåˆ—æ ‡æ³¨ï¼šæ¯ä¸ª token è¾“å‡º + çº¿æ€§å±‚
- é—®ç­”ä»»åŠ¡ï¼šé¢„æµ‹ç­”æ¡ˆèµ·å§‹å’Œç»“æŸä½ç½®

**è¶…å‚æ•°**ï¼š
- Batch size: 16, 32
- Learning rate: 5e-5, 3e-5, 2e-5
- Epochs: 2-4

## ğŸ”— ç›¸å…³è®ºæ–‡

- [Attention Is All You Need](../machine-learning/attention-is-all-you-need.md) - Transformer æ¶æ„åŸºç¡€
- **ELMo**: Deep contextualized word representations - æ—©æœŸçš„åŒå‘é¢„è®­ç»ƒ
- **GPT**: Improving Language Understanding by Generative Pre-Training - å•å‘é¢„è®­ç»ƒ
- **RoBERTa**: A Robustly Optimized BERT Pretraining Approach - BERT æ”¹è¿›
- **ELECTRA**: Pre-training Text Encoders as Discriminators - æ›´é«˜æ•ˆçš„é¢„è®­ç»ƒ

## ğŸ“Œ å…³é”®ä»£ç ç‰‡æ®µ

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# å‡†å¤‡è¾“å…¥
text = "BERT is a powerful pre-trained model."
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

# å‰å‘ä¼ æ’­
outputs = model(**inputs)
logits = outputs.logits
predictions = torch.argmax(logits, dim=-1)

print(f"Predictions: {predictions}")
```

### å¾®è°ƒç¤ºä¾‹

```python
from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# åŠ è½½æ•°æ®é›†
dataset = load_dataset('glue', 'sst2')

# å‡†å¤‡æ¨¡å‹
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=2e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

# è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['validation'],
)

trainer.train()
```

---

*é˜…è¯»æ—¥æœŸï¼š2024å¹´*
*ç¬”è®°çŠ¶æ€ï¼šå·²å®Œæˆ*
