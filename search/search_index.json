{"config":{"lang":["zh","en"],"separator":"[\\s\\-\\.]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\u6b22\u8fce\u6765\u5230\u8bba\u6587\u7b14\u8bb0","text":"<p>\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bb0\u5f55\u548c\u6574\u7406\u8bba\u6587\u9605\u8bfb\u7b14\u8bb0\u7684\u4e2a\u4eba\u77e5\u8bc6\u5e93\u3002</p>"},{"location":"#_2","title":"\ud83c\udfaf \u7f51\u7ad9\u7b80\u4ecb","text":"<p>\u672c\u7ad9\u70b9\u8bb0\u5f55\u4e86\u6211\u5728\u4eba\u5de5\u667a\u80fd\u3001\u673a\u5668\u5b66\u4e60\u7b49\u9886\u57df\u7684\u8bba\u6587\u9605\u8bfb\u7b14\u8bb0\u3002\u6bcf\u7bc7\u7b14\u8bb0\u5305\u542b\u8bba\u6587\u7684\u6838\u5fc3\u601d\u60f3\u3001\u65b9\u6cd5\u603b\u7ed3\u3001\u5b9e\u9a8c\u7ed3\u679c\u5206\u6790\u4ee5\u53ca\u4e2a\u4eba\u601d\u8003\u3002</p>"},{"location":"#_3","title":"\ud83d\udcda \u5185\u5bb9\u5206\u7c7b","text":"<p>\u7f51\u7ad9\u6309\u7814\u7a76\u9886\u57df\u7ec4\u7ec7\u8bba\u6587\u7b14\u8bb0\uff1a</p>"},{"location":"#simulation","title":"Simulation","text":"<p>Papers on performance modeling, simulation frameworks, and analytical models for ML systems.</p>"},{"location":"#autotuning","title":"Autotuning","text":"<p>Papers on automatic performance tuning, compiler optimization, and ML-guided program optimization.</p>"},{"location":"#intra-gpu-spatial-multiplexing","title":"Intra-GPU Spatial Multiplexing","text":"<p>Papers on GPU spatial multiplexing techniques for efficient LLM serving, including prefill-decode disaggregation and dynamic resource partitioning.</p>"},{"location":"#_4","title":"\ud83d\udd0d \u5982\u4f55\u4f7f\u7528","text":"<ul> <li>\u6d4f\u89c8\u5206\u7c7b\uff1a\u70b9\u51fb\u9876\u90e8\u5bfc\u822a\u680f\u9009\u62e9\u611f\u5174\u8da3\u7684\u7814\u7a76\u9886\u57df</li> <li>\u641c\u7d22\u8bba\u6587\uff1a\u4f7f\u7528\u53f3\u4e0a\u89d2\u7684\u641c\u7d22\u6846\u5feb\u901f\u67e5\u627e\u7279\u5b9a\u8bba\u6587\u6216\u5173\u952e\u8bcd</li> <li>\u9605\u8bfb\u7b14\u8bb0\uff1a\u6bcf\u7bc7\u7b14\u8bb0\u5305\u542b\u8bba\u6587\u6458\u8981\u3001\u6838\u5fc3\u601d\u60f3\u3001\u65b9\u6cd5\u4ecb\u7ecd\u548c\u4e2a\u4eba\u7406\u89e3</li> </ul>"},{"location":"#_5","title":"\ud83d\udcdd \u7b14\u8bb0\u6a21\u677f","text":"<p>\u6bcf\u7bc7\u8bba\u6587\u7b14\u8bb0\u901a\u5e38\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9\uff1a</p> <ul> <li>\u8bba\u6587\u57fa\u672c\u4fe1\u606f\uff08\u6807\u9898\u3001\u4f5c\u8005\u3001\u4f1a\u8bae/\u671f\u520a\u3001\u5e74\u4efd\uff09</li> <li>\u6458\u8981\uff1a\u8bba\u6587\u8981\u89e3\u51b3\u7684\u95ee\u9898\u548c\u4e3b\u8981\u8d21\u732e</li> <li>\u6838\u5fc3\u601d\u60f3\uff1a\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u70b9</li> <li>\u65b9\u6cd5\uff1a\u6280\u672f\u7ec6\u8282\u548c\u5b9e\u73b0\u65b9\u5f0f</li> <li>\u5b9e\u9a8c\u7ed3\u679c\uff1a\u4e3b\u8981\u5b9e\u9a8c\u7ed3\u8bba</li> <li>\u4e2a\u4eba\u7b14\u8bb0\uff1a\u4e2a\u4eba\u601d\u8003\u3001\u7591\u95ee\u548c\u5ef6\u4f38\u9605\u8bfb</li> </ul>"},{"location":"#_6","title":"\ud83d\ude80 \u66f4\u65b0\u65e5\u5fd7","text":"<p>\u672c\u7f51\u7ad9\u4f1a\u6301\u7eed\u66f4\u65b0\uff0c\u8bb0\u5f55\u6700\u65b0\u9605\u8bfb\u7684\u8bba\u6587\u3002\u6b22\u8fce\u8bbf\u95ee GitHub \u4ed3\u5e93 \u67e5\u770b\u66f4\u65b0\u5386\u53f2\u3002</p> <p>\u6700\u540e\u66f4\u65b0\uff1a2025\u5e74</p>"},{"location":"about/","title":"\u5173\u4e8e\u672c\u7ad9","text":""},{"location":"about/#_2","title":"\ud83c\udf93 \u5173\u4e8e\u4f5c\u8005","text":"<p>\u6211\u662f\u4e00\u540d\u5bf9\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u5145\u6ee1\u70ed\u60c5\u7684\u7814\u7a76\u8005/\u5b66\u4e60\u8005\u3002\u8fd9\u4e2a\u7f51\u7ad9\u662f\u6211\u6574\u7406\u8bba\u6587\u9605\u8bfb\u7b14\u8bb0\u3001\u6c89\u6dc0\u77e5\u8bc6\u7684\u4e2a\u4eba\u7a7a\u95f4\u3002</p>"},{"location":"about/#_3","title":"\ud83d\udca1 \u521b\u5efa\u521d\u8877","text":"<p>\u5728\u9605\u8bfb\u5927\u91cf\u8bba\u6587\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6211\u53d1\u73b0\uff1a</p> <ul> <li>\u8bba\u6587\u6570\u91cf\u591a\uff0c\u5bb9\u6613\u9057\u5fd8\u4e4b\u524d\u8bfb\u8fc7\u7684\u5185\u5bb9</li> <li>\u7f3a\u5c11\u7cfb\u7edf\u5316\u7684\u6574\u7406\uff0c\u96be\u4ee5\u5efa\u7acb\u77e5\u8bc6\u4f53\u7cfb</li> <li>\u597d\u7684\u60f3\u6cd5\u548c\u542f\u53d1\u9700\u8981\u8bb0\u5f55\u4e0b\u6765</li> </ul> <p>\u56e0\u6b64\u521b\u5efa\u4e86\u8fd9\u4e2a\u7f51\u7ad9\uff0c\u5e0c\u671b\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u7b14\u8bb0\u8bb0\u5f55\uff1a</p> <ol> <li>\u52a0\u6df1\u7406\u89e3\uff1a\u901a\u8fc7\u6574\u7406\u7b14\u8bb0\u5f3a\u5316\u5bf9\u8bba\u6587\u7684\u7406\u89e3</li> <li>\u77e5\u8bc6\u6c89\u6dc0\uff1a\u6784\u5efa\u4e2a\u4eba\u7684\u77e5\u8bc6\u5e93\uff0c\u65b9\u4fbf\u65e5\u540e\u67e5\u9605</li> <li>\u601d\u7ef4\u78b0\u649e\uff1a\u8bb0\u5f55\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u7684\u601d\u8003\u548c\u542f\u53d1</li> </ol>"},{"location":"about/#_4","title":"\ud83d\udee0\ufe0f \u6280\u672f\u6808","text":"<p>\u672c\u7ad9\u70b9\u4f7f\u7528\u4ee5\u4e0b\u6280\u672f\u6784\u5efa\uff1a</p> <ul> <li>MkDocs\uff1a\u9759\u6001\u7ad9\u70b9\u751f\u6210\u5668</li> <li>Material for MkDocs\uff1a\u73b0\u4ee3\u5316\u7684\u4e3b\u9898</li> <li>GitHub Pages\uff1a\u514d\u8d39\u6258\u7ba1</li> <li>GitHub Actions\uff1a\u81ea\u52a8\u5316\u90e8\u7f72</li> </ul>"},{"location":"about/#_5","title":"\ud83d\udcd6 \u7b14\u8bb0\u89c4\u8303","text":"<p>\u4e3a\u4e86\u4fdd\u6301\u7b14\u8bb0\u7684\u4e00\u81f4\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u6211\u9075\u5faa\u4ee5\u4e0b\u89c4\u8303\uff1a</p> <ul> <li>\u5ba2\u89c2\u8bb0\u5f55\uff1a\u51c6\u786e\u8bb0\u5f55\u8bba\u6587\u5185\u5bb9\uff0c\u4e0d\u6b6a\u66f2\u539f\u610f</li> <li>\u7ed3\u6784\u6e05\u6670\uff1a\u4f7f\u7528\u7edf\u4e00\u7684\u6a21\u677f\u7ec4\u7ec7\u5185\u5bb9</li> <li>\u91cd\u70b9\u7a81\u51fa\uff1a\u6807\u6ce8\u6838\u5fc3\u8d21\u732e\u548c\u5173\u952e\u521b\u65b0</li> <li>\u4e2a\u4eba\u601d\u8003\uff1a\u533a\u5206\u8bba\u6587\u5185\u5bb9\u548c\u4e2a\u4eba\u7406\u89e3</li> </ul>"},{"location":"about/#_6","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>GitHub \u4ed3\u5e93</li> </ul>"},{"location":"about/#_7","title":"\ud83d\udcc4 \u8bb8\u53ef\u534f\u8bae","text":"<p>\u672c\u7ad9\u70b9\u7684\u6240\u6709\u539f\u521b\u5185\u5bb9\u91c7\u7528 CC BY-NC-SA 4.0 \u8bb8\u53ef\u534f\u8bae\u3002</p> <p>\u8bba\u6587\u7248\u6743\u5f52\u539f\u4f5c\u8005\u6240\u6709\uff0c\u672c\u7ad9\u4ec5\u63d0\u4f9b\u5b66\u4e60\u7b14\u8bb0\u548c\u4e2a\u4eba\u7406\u89e3\u3002</p> <p>\u5982\u6709\u4efb\u4f55\u95ee\u9898\u6216\u5efa\u8bae\uff0c\u6b22\u8fce\u901a\u8fc7 GitHub Issues \u8054\u7cfb\u6211\uff01</p>"},{"location":"autotuning/","title":"Autotuning","text":"<p>Papers on automatic performance tuning, compiler optimization, and machine learning-guided optimization for tensor programs and GPU kernels.</p>"},{"location":"autotuning/#research-areas","title":"Research Areas","text":"<ul> <li>Tensor program optimization</li> <li>Cost models and performance prediction</li> <li>Search algorithms for program optimization</li> <li>GPU kernel optimization</li> <li>Compiler autotuning</li> </ul>"},{"location":"autotuning/#papers","title":"Papers","text":""},{"location":"autotuning/#tensor-program-optimization","title":"Tensor Program Optimization","text":"<ul> <li>Ansor - Automatic tensor program generation for deep learning (OSDI 2020, TVM)</li> <li>Learning to Optimize Tensor Programs - AutoTVM framework (NeurIPS 2018)</li> </ul>"},{"location":"autotuning/#cost-models-performance-prediction","title":"Cost Models &amp; Performance Prediction","text":"<ul> <li>MetaTune - Meta-learning based cost model for fast autotuning (2021)</li> <li>Learned Performance Model for TPU - ML-based TPU performance modeling (Google, 2020)</li> </ul>"},{"location":"autotuning/#gpu-kernel-optimization-portability","title":"GPU Kernel Optimization &amp; Portability","text":"<ul> <li>GPU Performance Portability - Autotuning for cross-vendor GPU portability (2025)</li> <li>Anatomy of Triton Attention Kernel - Building high-performance attention kernels with Triton (2025)</li> </ul> <p>Continuously updated...</p>"},{"location":"autotuning/anatomy-triton-attention/","title":"The Anatomy of a Triton Attention Kernel","text":"<p>Authors: Burkhard Ringlein, Jan van Lunteren, Radu Stoica Institution: IBM Research Europe Conference: arXiv 2025 Paper Link: arXiv:2511.11581</p> <p>Triton Attention Mechanism GPU Kernels</p>"},{"location":"autotuning/anatomy-triton-attention/#abstract","title":"Abstract","text":"<p>This paper develops a state-of-the-art paged attention kernel using exclusively OpenAI's Triton language, achieving state-of-the-art performance on both NVIDIA and AMD GPUs. It brings Triton attention kernel performance from 19.7% to 105.9% of state-of-the-art through systematic optimizations.</p> <p>Key Contributions: - Feature-complete cross-platform paged attention kernel - High-level approach with algorithmic and system-level improvements - Parameter auto-tuning for optimal performance - Open-sourced kernels adopted as default in vLLM for AMD GPUs</p>"},{"location":"autotuning/anatomy-triton-attention/#core-ideas","title":"Core Ideas","text":"<p>The paper demonstrates how to build production-quality attention kernels using high-level Triton code through systematic optimization. Triton enables writing GPU kernels in Python while abstracting away CUDA threading complexities like memory coalescing and tensor core scheduling. The resulting kernel achieves competitive or better performance than vendor-optimized implementations while maintaining cross-platform portability.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"autotuning/ansor/","title":"Ansor: Generating High-Performance Tensor Programs for Deep Learning","text":"<p>Authors: Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, Joseph E. Gonzalez, Ion Stoica Institution: UC Berkeley, University of Washington, AWS Conference: OSDI 2020 Paper Link: arXiv:2006.06762</p> <p>Tensor Programs Auto-Scheduling TVM</p>"},{"location":"autotuning/ansor/#abstract","title":"Abstract","text":"<p>Ansor is a tensor program generation framework for deep learning that automatically generates high-performance code without manual templates. It explores a large search space by sampling from a hierarchical representation and fine-tunes programs with evolutionary search and learned cost models.</p> <p>Key Contributions: - Template-free automatic tensor program generation - Hierarchical search space sampling with evolutionary search - Learned cost model for program performance prediction - Improves performance by up to 3.8\u00d7 (Intel CPU), 2.6\u00d7 (ARM CPU), 1.7\u00d7 (NVIDIA GPU)</p>"},{"location":"autotuning/ansor/#core-ideas","title":"Core Ideas","text":"<p>Unlike AutoTVM which requires manual templates, Ansor takes only tensor expressions as input and generates high-performance code automatically. It has three major components: a program sampler for diverse program generation, a performance tuner using evolutionary search, and a task scheduler for time resource allocation. Ansor is integrated into Apache TVM as the tvm.auto_scheduler package.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"autotuning/gpu-performance-portability/","title":"GPU Performance Portability needs Autotuning","text":"<p>Authors: Burkhard Ringlein, Thomas Parnell, Radu Stoica Institution: IBM Research Europe Conference: arXiv 2025 Paper Link: arXiv:2505.03780</p> <p>GPU Portability Autotuning LLM Inference</p>"},{"location":"autotuning/gpu-performance-portability/#abstract","title":"Abstract","text":"<p>This paper makes the case for combining just-in-time (JIT) compilation with comprehensive kernel parameter autotuning to enable portable LLM inference with state-of-the-art performance without code changes, addressing vendor lock-in and hardware portability challenges.</p> <p>Key Contributions: - Explores up to 15\u00d7 more kernel parameter configurations - Produces significantly more diverse code across multiple dimensions - Outperforms vendor-optimized implementations by up to 230% - Reduces kernel code size by 70\u00d7 while eliminating manual optimizations</p>"},{"location":"autotuning/gpu-performance-portability/#core-ideas","title":"Core Ideas","text":"<p>As LLMs grow in complexity, reliance on a single dominant platform limits portability and creates vendor lock-in. Combining JIT compilation with comprehensive autotuning enables performance-portable LLM inference across different GPU vendors. The \"dejavu\" mechanism for Triton's autotuner reduces overhead to zero, enabling production use with full autotuning benefits.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"autotuning/learned-performance-model-tpu/","title":"A Learned Performance Model for Tensor Processing Units","text":"<p>Authors: Samuel J. Kaufman, Phitchaya Mangpo Phothilimthana, Yanqi Zhou Institution: Google Conference: MLSys 2021 Paper Link: arXiv:2008.01040</p> <p>TPU Performance Modeling Machine Learning</p>"},{"location":"autotuning/learned-performance-model-tpu/#abstract","title":"Abstract","text":"<p>This paper demonstrates a method of learning performance models from a corpus of tensor computation graph programs for Tensor Processing Unit (TPU) instances. The learned model outperforms heavily-optimized analytical performance models on tile-size selection and operator fusion tasks.</p> <p>Key Contributions: - Machine learning-based performance modeling for TPUs - Outperforms analytical models on key optimization tasks - Enables autotuning in settings where TPU access is limited or expensive - Reduces development burden for modeling contemporary accelerators</p>"},{"location":"autotuning/learned-performance-model-tpu/#core-ideas","title":"Core Ideas","text":"<p>Accurate hardware performance models are critical for efficient code generation but difficult to develop for complex processors. This research shows that learning from execution data can produce more accurate models than traditional analytical approaches, particularly useful for the proliferation of deep learning accelerators where analytical modeling is costly.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"autotuning/learning-optimize-tensor-programs/","title":"Learning to Optimize Tensor Programs","text":"<p>Authors: Tianqi Chen, Lianmin Zheng, Eddie Q. Yan, Ziheng Jiang, Thierry Moreau, et al. Institution: University of Washington, AWS, Cornell Conference: NeurIPS 2018 Paper Link: arXiv:1805.08166</p> <p>AutoTVM Machine Learning Compiler Optimization</p>"},{"location":"autotuning/learning-optimize-tensor-programs/#abstract","title":"Abstract","text":"<p>This paper introduces AutoTVM, a learning-based framework to optimize tensor programs for deep learning workloads. It learns domain-specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants.</p> <p>Key Contributions: - Machine learning-based tensor program optimization framework - Statistical cost models for guiding program search - Transfer learning acceleration (2-10\u00d7 speedup) - End-to-end performance improvements of 1.2\u00d7 to 3.8\u00d7</p>"},{"location":"autotuning/learning-optimize-tensor-programs/#core-ideas","title":"Core Ideas","text":"<p>AutoTVM addresses the limitation of manually optimized libraries like cuDNN that only support narrow ranges of hardware. It generates programs competitive with hardware-specific libraries while enabling operator fusion optimizations impossible with fixed operator libraries. The framework formalizes the optimization problem and proposes an ML solution that generalizes across different hardware targets.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"autotuning/metatune/","title":"MetaTune: Meta-Learning Based Cost Model for Fast and Efficient Auto-tuning Frameworks","text":"<p>Authors: Jaehun Ryu, Hyojin Sung Institution: Samsung Electronics, Seoul National University Conference: arXiv 2021 Paper Link: arXiv:2102.04199</p> <p>Meta-Learning Cost Model TVM</p>"},{"location":"autotuning/metatune/#abstract","title":"Abstract","text":"<p>MetaTune is a meta-learning based cost model that quickly and accurately predicts the performance of optimized codes with pre-trained model parameters. It addresses the large space exploration and cost model training overheads in auto-tuning frameworks.</p> <p>Key Contributions: - GNN-based meta-learning for cost model training - Encodes convolution kernels as structurally similar graphs - 8-13% better inference time on average for four CNN models - Outperforms transfer learning by 10% in cross-platform cases</p>"},{"location":"autotuning/metatune/#core-ideas","title":"Core Ideas","text":"<p>MetaTune encodes convolution kernel codes as graphs to facilitate meta-learning, meta-trains a GNN model with minimal input data, and predicts optimization parameters for unseen operations during compilation. Implemented as an alternative cost model for TVM's auto-tuning framework, it provides comparable or lower optimization time while improving inference performance.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"intra-gpu-multiplexing/","title":"Intra-GPU Spatial Multiplexing","text":"<p>Papers on GPU spatial multiplexing techniques for efficient LLM serving, including prefill-decode disaggregation and dynamic resource partitioning.</p>"},{"location":"intra-gpu-multiplexing/#research-areas","title":"Research Areas","text":"<ul> <li>Prefill and decode phase disaggregation</li> <li>GPU SM (Streaming Multiprocessor) partitioning</li> <li>Spatial-temporal resource orchestration</li> <li>Dynamic workload scheduling</li> <li>SLO-oriented serving optimization</li> </ul>"},{"location":"intra-gpu-multiplexing/#papers","title":"Papers","text":""},{"location":"intra-gpu-multiplexing/#spatial-multiplexing-frameworks","title":"Spatial Multiplexing Frameworks","text":"<ul> <li>NanoFlow - Optimal LLM serving with intra-device parallelism</li> <li>Bullet - Dynamic spatial-temporal orchestration for GPU utilization</li> <li>Nexus - Proactive intra-GPU disaggregation of prefill and decode</li> <li>PD-Multiplexing - SLO-oriented LLM serving with phase multiplexing</li> </ul>"},{"location":"intra-gpu-multiplexing/bullet/","title":"Bullet: Boosting GPU Utilization for LLM Serving via Dynamic Spatial-Temporal Orchestration","text":"<p>Authors: Zejia Lin, Hongxin Xu, Guanyi Chen Institution: Sun Yat-sen University Conference: arXiv 2025 Paper Link: arXiv:2504.19516</p> <p>GPU Utilization Spatial-Temporal Orchestration Performance Modeling</p>"},{"location":"intra-gpu-multiplexing/bullet/#abstract","title":"Abstract","text":"<p>Bullet is a novel spatial-temporal orchestration system that eliminates GPU inefficiencies through precise phase coordination. It enables concurrent execution of prefill and decode phases while dynamically provisioning GPU resources using real-time performance modeling. Bullet delivers 1.26x average throughput gains (up to 1.55x) over state-of-the-art systems while consistently meeting latency constraints.</p> <p>Key Contributions: - Dynamic spatial-temporal GPU resource orchestration - Concurrent prefill and decode execution - Real-time performance modeling for resource provisioning - 1.26x average throughput improvement (up to 1.55x)</p>"},{"location":"intra-gpu-multiplexing/bullet/#core-ideas","title":"Core Ideas","text":"<p>Bullet addresses the fundamental mismatch between compute-intensive prefill and memory-bound decode phases in LLM serving. The system identifies two key inefficiencies: (1) prefill suffers from suboptimal compute utilization due to wave quantization and attention bottlenecks, and (2) hybrid batches prioritize latency over throughput. Bullet's spatial-temporal orchestration dynamically coordinates phase execution and resource allocation to maximize GPU utilization while meeting latency requirements.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"intra-gpu-multiplexing/nanoflow/","title":"NanoFlow: Towards Optimal Large Language Model Serving Throughput","text":"<p>Authors: Kan Zhu, Liangyu Zhao, Yile Gu Institution: University of Washington Conference: OSDI 2025 Paper Link: arXiv:2408.12757</p> <p>LLM Serving Intra-Device Parallelism GPU Optimization</p>"},{"location":"intra-gpu-multiplexing/nanoflow/#abstract","title":"Abstract","text":"<p>NanoFlow is a novel serving framework that exploits intra-device parallelism to achieve optimal LLM serving throughput. By overlapping the usage of heterogeneous resources within a single GPU device, NanoFlow provides 1.91x throughput boost and achieves 68.5% of optimal throughput.</p> <p>Key Contributions: - Intra-device parallelism framework for LLM serving - Overlapping heterogeneous resource usage within single GPU - 1.91x throughput improvement over baseline systems - Achieves 68.5% of theoretical optimal throughput</p>"},{"location":"intra-gpu-multiplexing/nanoflow/#core-ideas","title":"Core Ideas","text":"<p>NanoFlow addresses the inefficiency in current LLM serving systems by exploiting parallelism opportunities within a single GPU device. Unlike previous approaches that focus on inter-device parallelism, NanoFlow carefully orchestrates compute, memory, and communication resources to maximize utilization through fine-grained intra-device scheduling.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"intra-gpu-multiplexing/nexus/","title":"Nexus: Proactive Intra-GPU Disaggregation of Prefill and Decode in LLM Serving","text":"<p>Authors: Xiaoxiang Shi, Colin Cai, Junjia Du, Zhihao Jia Institution: Carnegie Mellon University Conference: arXiv 2025 Paper Link: arXiv:2507.06608</p> <p>Prefill-Decode Disaggregation GPU Multiplexing Dynamic Workload</p>"},{"location":"intra-gpu-multiplexing/nexus/#abstract","title":"Abstract","text":"<p>Nexus achieves proactive intra-GPU disaggregation that adapts effectively to dynamic workloads by managing the conflicting resource demands of prefill and decode phases under varying conditions. It achieves up to 2.2\u00d7 lower time-between-tokens (TBT) and 1.4\u00d7 higher throughput than vLLM-disaggregation with only half the number of GPUs.</p> <p>Key Contributions: - Proactive intra-GPU disaggregation framework - Adaptive resource management for prefill and decode phases - 2.2\u00d7 lower TBT compared to vLLM-disaggregation - 1.4\u00d7 higher throughput with 50% fewer GPUs</p>"},{"location":"intra-gpu-multiplexing/nexus/#core-ideas","title":"Core Ideas","text":"<p>Nexus extends vLLM with fine-grained resource partitioning to enable efficient coexistence of prefill and decode operations on the same GPU. The key insight is managing the dynamic and conflicting resource demands proactively rather than reactively, allowing the system to adapt to varying workload characteristics while maintaining high throughput and low latency.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"intra-gpu-multiplexing/pd-multiplex/","title":"Optimizing SLO-oriented LLM Serving with PD-Multiplexing","text":"<p>Authors: Weihao Cui, Yukang Chen, Han Zhao Institution: Shanghai Jiao Tong University, Huawei Cloud Conference: arXiv 2025 Paper Link: arXiv:2504.14489</p> <p>SLO Optimization PD Multiplexing GPU Partitioning</p>"},{"location":"intra-gpu-multiplexing/pd-multiplex/#abstract","title":"Abstract","text":"<p>This paper presents Drift, an LLM serving framework that resolves system tensions via PD (Prefill-Decode) multiplexing, enabling in-place and phase-decoupled compute partition. Drift leverages low-level GPU partitioning techniques to multiplex prefill and decode phases spatially and adaptively on shared GPUs while preserving in-place memory sharing.</p> <p>Key Contributions: - Drift framework for SLO-oriented LLM serving - In-place and phase-decoupled compute partitioning - Spatial and adaptive multiplexing of prefill/decode phases - Low-level GPU partitioning with memory sharing preservation</p>"},{"location":"intra-gpu-multiplexing/pd-multiplex/#core-ideas","title":"Core Ideas","text":"<p>Drift addresses the challenge of meeting Service Level Objectives (SLOs) in LLM serving by introducing PD-multiplexing. The system uses low-level GPU partitioning to spatially multiplex compute-intensive prefill operations with memory-bound decode operations on the same GPU, adaptively adjusting resource allocation based on workload characteristics while maintaining efficient memory sharing.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/","title":"Simulation","text":"<p>Papers on performance modeling, simulation frameworks, and analytical models for machine learning systems.</p>"},{"location":"simulation/#research-areas","title":"Research Areas","text":"<ul> <li>Hardware simulation for ML workloads</li> <li>Performance modeling and prediction</li> <li>Analytical models for inference/training</li> <li>Trace-based simulation</li> <li>Co-design and architecture exploration</li> </ul>"},{"location":"simulation/#papers","title":"Papers","text":""},{"location":"simulation/#performance-modeling-prediction","title":"Performance Modeling &amp; Prediction","text":"<ul> <li>AMALI - Analytical model for LLM inference on modern GPUs (ISCA 2025)</li> <li>NeuSight - Forecasting GPU performance for deep learning training and inference (ASPLOS 2025)</li> <li>LLMCompass - Hardware evaluation framework for LLM inference</li> <li>Path Forward Beyond Simulators - Fast and accurate GPU execution time prediction for DNN workloads (MICRO 2023)</li> </ul>"},{"location":"simulation/#llm-inference-serving-simulation","title":"LLM Inference &amp; Serving Simulation","text":"<ul> <li>VIDUR - Large-scale simulation framework for LLM inference (MLSys 2024, Microsoft)</li> <li>Frontier - Simulating next-generation LLM inference systems with MoE and disaggregation</li> <li>APEX - Extensible and dynamism-aware simulator for LLM serving</li> </ul>"},{"location":"simulation/#training-simulation-optimization","title":"Training Simulation &amp; Optimization","text":"<ul> <li>vTrain - Simulation framework for cost-effective LLM training (MICRO 2024)</li> <li>Astra-Sim - Distributed training simulator (Intel, Meta, Georgia Tech)</li> <li>Astra-Sim 2.0 - Enhanced version with hierarchical networks and disaggregated systems</li> <li>SimAI - Unifying architecture design and performance tuning for large-scale LLM training (NSDI 2025, Alibaba)</li> </ul>"},{"location":"simulation/#benchmarking-tracing","title":"Benchmarking &amp; Tracing","text":"<ul> <li>Chakra - Execution trace format for ML workloads (Meta)</li> <li>Mystique - Production AI benchmark generation (ISCA 2023)</li> </ul> <p>Continuously updated...</p>"},{"location":"simulation/amali/","title":"AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs","text":"<p>Authors: Shiheng Cao, Junmin Wu, Junshi Chen Institution: University of Science and Technology of China (USTC) Conference: ISCA 2025 Paper Link: ACM DL</p> <p>Performance Modeling LLM Inference GPU Architecture</p>"},{"location":"simulation/amali/#abstract","title":"Abstract","text":"<p>AMALI addresses the challenge of accurately modeling LLM inference workloads on modern GPUs. The model provides detailed performance prediction for LLM inference by incorporating specialized models for tensor cores, constant cache, and instruction cache.</p> <p>Key Contributions: - Instruction modifier and throughput-based tensor core model that captures math pipe throttle stalls - Analytical models for constant cache and instruction cache developed using micro-benchmarks - Multi-warp model leveraging warp instruction number distribution for LLM inference characteristics</p>"},{"location":"simulation/amali/#core-ideas","title":"Core Ideas","text":"<p>AMALI reduces mean absolute percentage error (MAPE) from 127.56% to 23.59% compared to the state-of-the-art GCoM model when validated on an A100 GPU. The framework can also be used for architecture design space exploration, accurately predicting end-to-end performance improvements with enhanced tensor core capability on H100.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/apex/","title":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving","text":"<p>Authors: Yi-Chien Lin, Woosuk Kwon, Ronald Pineda Institution: USC, UC Berkeley, UCLA Conference: arXiv 2024 Paper Link: arXiv:2411.17651</p> <p>LLM Serving Parallel Execution Performance Simulation</p>"},{"location":"simulation/apex/#abstract","title":"Abstract","text":"<p>APEX is an LLM serving system simulator that efficiently identifies optimal parallel execution plans by considering key factors such as memory usage and batching behavior. The simulator performs dynamism-aware simulation to model iteration-level batching and leverages LLMs' repetitive structure to reduce design space exploration.</p> <p>Key Contributions: - Dynamism-aware simulation for iteration-level batching in LLM serving - Efficient design space exploration leveraging LLM repetitive structure - Scales to trillion-scale models with optimal parallel execution planning</p>"},{"location":"simulation/apex/#core-ideas","title":"Core Ideas","text":"<p>APEX addresses the challenge of selecting optimal parallel execution plans by balancing computation, memory, and communication overhead. It efficiently handles varying parallelism techniques (data, pipeline, tensor) and workload characteristics to identify the best execution strategy.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/astra-sim-2/","title":"ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems for Large-model Training at Scale","text":"<p>Authors: Collaboration between Intel, Meta, and Georgia Tech Institution: Intel, Meta, Georgia Tech Conference: IEEE 2023 Paper Link: arXiv:2303.14006</p> <p>Distributed Training Hierarchical Networks Disaggregated Systems</p>"},{"location":"simulation/astra-sim-2/#abstract","title":"Abstract","text":"<p>ASTRA-sim2.0 extends the open-source ASTRA-sim infrastructure with enhanced capabilities to model state-of-the-art and emerging distributed training models and platforms. It addresses the complex software/hardware co-design stack necessary for large-scale model training.</p> <p>Key Contributions: - Enhanced modeling of hierarchical network topologies - Support for disaggregated system architectures - Improved scalability for large-model training simulation</p>"},{"location":"simulation/astra-sim-2/#core-ideas","title":"Core Ideas","text":"<p>ASTRA-sim2.0 extends the original framework to handle modern training paradigms including hierarchical networks and disaggregated memory/compute systems. This enables more accurate simulation of emerging training platforms needed for models with billions to trillions of parameters.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/astra-sim/","title":"ASTRA-sim: Enabling SW/HW Co-Design Exploration for Distributed DL Training Platforms","text":"<p>Authors: Collaboration between Intel, Meta, and Georgia Tech Institution: Intel, Meta, Georgia Tech Conference: IEEE/Multiple Conferences Paper Link: ASTRA-sim Website</p> <p>Distributed Training Network Simulation Hardware-Software Co-design</p>"},{"location":"simulation/astra-sim/#abstract","title":"Abstract","text":"<p>ASTRA-sim is a distributed machine learning system simulator that models the end-to-end software and hardware stack of modern AI systems, including workload scheduling, collective communication algorithms, and hardware architectures (compute/memory/network).</p> <p>Key Contributions: - End-to-end cycle-accurate distributed training simulator - Support for multiple compute models (roofline, SCALE-sim) and network models (analytical, Garnet, NS3) - Enables systematic study of bottlenecks for scaling training of large models</p>"},{"location":"simulation/astra-sim/#core-ideas","title":"Core Ideas","text":"<p>ASTRA-sim enables design-space exploration for running large DNN models over future training platforms. It supports simulation from simple analytical to detailed cycle-accurate modeling of large-scale training platforms, particularly relevant for models like GPT-3 or DLRM that need to scale across thousands of accelerator nodes.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/chakra/","title":"Chakra: Execution Traces for Benchmarking and Network Performance Optimization","text":"<p>Authors: Meta AI Research Institution: Meta (Facebook) Conference: Engineering at Meta 2023 Paper Link: Meta Engineering Blog</p> <p>Execution Traces Benchmarking Performance Analysis</p>"},{"location":"simulation/chakra/#abstract","title":"Abstract","text":"<p>Chakra introduces an open graph-based representation of AI/ML workload execution, laying the foundation for benchmarking and network performance optimization. It provides a standardized schema for performance modeling through execution traces.</p> <p>Key Contributions: - Graph-based representation of AI/ML workload execution - Standardized Chakra execution trace format - Captures compute, communication, and memory operations with timing and dependencies - Integration with MLCommons for industry-wide adoption</p>"},{"location":"simulation/chakra/#core-ideas","title":"Core Ideas","text":"<p>Chakra execution traces capture arbitrary distributed ML workloads using a directed acyclic graph (DAG) representation of compute, communication, and remote memory nodes. In collaboration with MLCommons, Meta has open-sourced tools to enable collection, analysis, generation, and adoption of Chakra traces by simulators, emulators, and replay tools.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/frontier/","title":"Frontier: Simulating the Next Generation of LLM Inference Systems","text":"<p>Authors: Yicheng Feng, Xin Tan, Kin Hang Sew Institution: CUHK, ByteDance Conference: arXiv 2025 Paper Link: arXiv:2508.03148</p> <p>LLM Inference MoE Models Disaggregated Systems</p>"},{"location":"simulation/frontier/#abstract","title":"Abstract","text":"<p>Frontier is a high-fidelity simulator designed for next-generation LLM inference systems. It handles the growing complexity of Mixture-of-Experts (MoE) models and disaggregated architectures that decouple components like prefill/decode or attention/FFN for heterogeneous scaling.</p> <p>Key Contributions: - Unified framework for both co-located and disaggregated systems - Native support for MoE inference with expert parallelism - Simulation of complex workflows like cross-cluster expert routing - Advanced pipelining strategies for latency hiding</p>"},{"location":"simulation/frontier/#core-ideas","title":"Core Ideas","text":"<p>Frontier follows event-driven and modular design principles with a central GlobalController and modular ClusterWorkers. It achieves predicted throughput within 19.0% to 23.2% relative error margin, providing accurate performance trends for system-of-systems nature of modern LLM serving.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/linear/","title":"Path Forward Beyond Simulators: Fast and Accurate GPU Execution Time Prediction for DNN Workloads","text":"<p>Authors: Ying Li, Yifan Sun, Adwait Jog Institution: Multiple Institutions Conference: MICRO 2023 Paper Link: ACM DL</p> <p>GPU Performance DNN Prediction Linear Regression</p>"},{"location":"simulation/linear/#abstract","title":"Abstract","text":"<p>This paper proposes a fast, linear-regression-based execution time predictor for DNNs as an alternative to slow simulators. It predicts new DNN performance with 7% error and new GPU performance with 15.2% error, offering a practical solution for modeling large-scale systems and DNNs.</p> <p>Key Contributions: - Fast linear-regression-based DNN execution time predictor - Multiple performance models: End-to-End (E2E), Layer-Wise (LW), Kernel-Wise (KW), and Inter-GPU Kernel-Wise (IGKW) - Large-scale dataset with 646 models from HuggingFace, TorchVision - Practical alternative to time-consuming simulators</p>"},{"location":"simulation/linear/#core-ideas","title":"Core Ideas","text":"<p>Simulators are becoming increasingly impractical for modeling today's large-scale systems and DNNs due to long simulation times. This work collects execution time data for DNNs, layers, and kernels on various GPUs, then develops lightweight regression models that maintain acceptable accuracy while dramatically reducing prediction time compared to full simulation approaches.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/llmcompass/","title":"LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference","text":"<p>Authors: Hengrui Zhang, August Ning, Rohan Baskar Prabhakar, David Wentzlaff Institution: Princeton University Conference: ISCA 2024 Paper Link: IEEE Xplore | GitHub</p> <p>Hardware Design LLM Inference Design Space Exploration</p>"},{"location":"simulation/llmcompass/#abstract","title":"Abstract","text":"<p>LLMCompass is a hardware evaluation framework for LLM inference workloads that is fast, accurate, versatile, and able to describe and evaluate different hardware designs. It includes an automatic mapper to find performance-optimal mapping and scheduling.</p> <p>Key Contributions: - Average 10.9% error rate for operator latency estimation, 4.1% for LLM inference - Area-based cost model for architectural design space exploration - Can simulate 4-NVIDIA A100 GPU node running GPT-3 175B in 16 minutes - Identifies designs achieving 3.41x improvement in performance/cost vs A100</p>"},{"location":"simulation/llmcompass/#core-ideas","title":"Core Ideas","text":"<p>Implemented as a Python library, LLMCompass democratizes hardware design space exploration research with fully open-source tools. It enables architects to reason about design choices through combined performance and area modeling, making it suitable for exploring cost-effective hardware accelerators for LLM workloads.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/mystique/","title":"Mystique: Enabling Accurate and Scalable Generation of Production AI Benchmarks","text":"<p>Authors: Mingyu Liang, Wenyin Fu, Louis Feng Institution: Cornell University, Meta Conference: ISCA 2023 Paper Link: ACM DL</p> <p>Benchmarking AI Workloads Performance Analysis</p>"},{"location":"simulation/mystique/#abstract","title":"Abstract","text":"<p>Mystique enables accurate and scalable generation of production AI benchmarks. It addresses the challenge of creating representative benchmarks that reflect real-world AI workload characteristics for performance evaluation.</p> <p>Key Contributions: - Scalable benchmark generation for production AI workloads - Accurate representation of real-world AI characteristics - Framework for systematic performance evaluation</p>"},{"location":"simulation/mystique/#core-ideas","title":"Core Ideas","text":"<p>Mystique provides a systematic approach to generating benchmarks that accurately represent production AI workloads, enabling more realistic performance evaluation of AI systems and hardware. This is crucial for bridging the gap between research benchmarks and real-world deployment scenarios.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/neusight/","title":"NeuSight: Forecasting GPU Performance for Deep Learning Training and Inference","text":"<p>Authors: Seonho Lee, Amar Phanishayee, Divya Mahajan Institution: Georgia Tech, Meta Conference: ASPLOS 2025 Paper Link: arXiv:2407.13853</p> <p>GPU Performance Deep Learning Performance Forecasting</p>"},{"location":"simulation/neusight/#abstract","title":"Abstract","text":"<p>NeuSight is a forecasting framework that predicts the performance of diverse deep learning models for both training and inference on unseen GPUs without requiring actual execution on the target GPU. It achieves exceptional accuracy compared to previous approaches.</p> <p>Key Contributions: - Reduces latency prediction error from 121.4% and 30.8% to 2.3% for GPT-3 on H100 - Inference error: 9.7% (vs prior work 220.9%), Training error: 7.3% (vs prior work 725.8%) - Tile-based approach predicting device utilization per tile - Works for both single-GPU and multi-GPU distributed settings</p>"},{"location":"simulation/neusight/#core-ideas","title":"Core Ideas","text":"<p>Unlike prior work that directly predicts kernel latency using machine learning, NeuSight distills kernel execution into tiles, predicts utilization per tile, and determines latency based on GPU architecture performance bounds. This novel approach significantly improves accuracy across various workloads and modern GPUs.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/simai/","title":"SimAI: Unifying Architecture Design and Performance Tuning for Large-Scale Large Language Model Training with Scalability and Precision","text":"<p>Authors: Xizheng Wang, Qingxu Li, Yichi Xu Institution: Alibaba Cloud, Tsinghua University Conference: NSDI 2025 Paper Link: USENIX | GitHub</p> <p>LLM Training Architecture Design Performance Tuning</p>"},{"location":"simulation/simai/#abstract","title":"Abstract","text":"<p>SimAI is the industry's first full-stack, high-precision simulator for AI large-scale training. It aims at precisely and efficiently simulating the LLM training procedure at scale, achieving an average of 98.1% alignment to real-world results under various test scenarios.</p> <p>Key Contributions: - High-fidelity integration of training frameworks, kernel computation, and collective communication - Multi-thread acceleration with lock-free global context-sharing - Three operation modes: Analytical, Simulation, and Physical - Comprehensive modeling of the entire LLM training process</p>"},{"location":"simulation/simai/#core-ideas","title":"Core Ideas","text":"<p>SimAI supports three major operation modes: (1) SimAI-Analytical for fast simulation using bus bandwidth abstraction, (2) SimAI-Simulation for full-stack simulation with fine-grained network communication modeling using NS3, and (3) SimAI-Physical for physical traffic generation in CPU RDMA cluster environments. The simulator provides detailed modeling of framework, collective communication, and network layers.</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/vidur/","title":"VIDUR: A Large-Scale Simulation Framework for LLM Inference","text":"<p>Authors: Amey Agrawal, Nitin Kedia, Jayashree Mohan Institution: Microsoft Research, IISc Bangalore Conference: MLSys 2024 Paper Link: arXiv:2405.05465</p> <p>LLM Inference Serving Simulation Performance Optimization</p>"},{"location":"simulation/vidur/#abstract","title":"Abstract","text":"<p>Vidur is a high-fidelity, extensible simulation framework for LLM inference performance that enables studying system performance and capacity planning without requiring expensive GPU access. It achieves less than 9% error in estimating inference latency across multiple models.</p> <p>Key Contributions: - High-fidelity simulation using experimental profiling and predictive modeling - Capacity planning and deployment configuration optimization - Testing new research ideas (scheduling algorithms, speculative decoding) - Validated on LLaMA2 7/70B, InternLM-20B, and Qwen-72B</p>"},{"location":"simulation/vidur/#core-ideas","title":"Core Ideas","text":"<p>Vidur combines experimental profiling with predictive modeling to evaluate end-to-end inference performance. The companion tool Vidur-Search automatically identifies cost-effective deployment configurations meeting performance constraints, finding optimal LLaMA2-70B configuration in 1 hour on CPU (vs 42K GPU hours costing $218K for deployment-based exploration).</p> <p>Reading date: 2025 Note status: Completed</p>"},{"location":"simulation/vtrain/","title":"vTrain: A Simulation Framework for Evaluating Cost-Effective and Compute-Optimal Large Language Model Training","text":"<p>Authors: Jehyeon Bang, Yujeong Choi, Myeongwoo Kim Institution: KAIST Conference: MICRO 2024 Paper Link: arXiv:2312.12391</p> <p>LLM Training Cost Optimization Training Simulation</p>"},{"location":"simulation/vtrain/#abstract","title":"Abstract","text":"<p>vTrain is a profiling-driven simulator providing a fast yet accurate software framework to determine efficient and cost-effective LLM training system configurations. It addresses the critical challenge of training large models cost-effectively by exploring the parallelization design space.</p> <p>Key Contributions: - Fast design space exploration (under 200 seconds for full exploration) - Identifies competitive training plans within minutes - Supports multi-tenant GPU cluster scheduling optimization - Determines compute-optimal model architectures given fixed compute budget</p>"},{"location":"simulation/vtrain/#core-ideas","title":"Core Ideas","text":"<p>vTrain addresses limitations of heuristic-based parallel training strategies that leave significant performance on the table, wasting millions of dollars in training costs. Through case studies, it demonstrates effectiveness in evaluating optimal parallelization strategies balancing training time and cost, efficient multi-tenant scheduling, and compute-optimal model architecture selection.</p> <p>Reading date: 2025 Note status: Completed</p>"}]}