{"config":{"lang":["zh","en"],"separator":"[\\s\\-\\.]+","pipeline":["stemmer"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\u6b22\u8fce\u6765\u5230\u8bba\u6587\u7b14\u8bb0","text":"<p>\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bb0\u5f55\u548c\u6574\u7406\u8bba\u6587\u9605\u8bfb\u7b14\u8bb0\u7684\u4e2a\u4eba\u77e5\u8bc6\u5e93\u3002</p>"},{"location":"#_2","title":"\ud83c\udfaf \u7f51\u7ad9\u7b80\u4ecb","text":"<p>\u672c\u7ad9\u70b9\u8bb0\u5f55\u4e86\u6211\u5728\u4eba\u5de5\u667a\u80fd\u3001\u673a\u5668\u5b66\u4e60\u7b49\u9886\u57df\u7684\u8bba\u6587\u9605\u8bfb\u7b14\u8bb0\u3002\u6bcf\u7bc7\u7b14\u8bb0\u5305\u542b\u8bba\u6587\u7684\u6838\u5fc3\u601d\u60f3\u3001\u65b9\u6cd5\u603b\u7ed3\u3001\u5b9e\u9a8c\u7ed3\u679c\u5206\u6790\u4ee5\u53ca\u4e2a\u4eba\u601d\u8003\u3002</p>"},{"location":"#_3","title":"\ud83d\udcda \u5185\u5bb9\u5206\u7c7b","text":"<p>\u7f51\u7ad9\u6309\u7814\u7a76\u9886\u57df\u7ec4\u7ec7\u8bba\u6587\u7b14\u8bb0\uff1a</p>"},{"location":"#_4","title":"\u673a\u5668\u5b66\u4e60","text":"<p>\u5305\u62ec\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u3001\u4f18\u5316\u65b9\u6cd5\u3001\u6a21\u578b\u67b6\u6784\u7b49\u76f8\u5173\u8bba\u6587\u3002</p>"},{"location":"#_5","title":"\u81ea\u7136\u8bed\u8a00\u5904\u7406","text":"<p>\u5305\u542b\u8bed\u8a00\u6a21\u578b\u3001\u6587\u672c\u7406\u89e3\u3001\u673a\u5668\u7ffb\u8bd1\u7b49NLP\u9886\u57df\u7684\u8bba\u6587\u3002</p>"},{"location":"#simulation","title":"Simulation","text":"<p>Papers on performance modeling, simulation frameworks, and analytical models for ML systems.</p>"},{"location":"#autotuning","title":"Autotuning","text":"<p>Papers on automatic performance tuning, compiler optimization, and ML-guided program optimization.</p>"},{"location":"#_6","title":"\ud83d\udd0d \u5982\u4f55\u4f7f\u7528","text":"<ul> <li>\u6d4f\u89c8\u5206\u7c7b\uff1a\u70b9\u51fb\u9876\u90e8\u5bfc\u822a\u680f\u9009\u62e9\u611f\u5174\u8da3\u7684\u7814\u7a76\u9886\u57df</li> <li>\u641c\u7d22\u8bba\u6587\uff1a\u4f7f\u7528\u53f3\u4e0a\u89d2\u7684\u641c\u7d22\u6846\u5feb\u901f\u67e5\u627e\u7279\u5b9a\u8bba\u6587\u6216\u5173\u952e\u8bcd</li> <li>\u9605\u8bfb\u7b14\u8bb0\uff1a\u6bcf\u7bc7\u7b14\u8bb0\u5305\u542b\u8bba\u6587\u6458\u8981\u3001\u6838\u5fc3\u601d\u60f3\u3001\u65b9\u6cd5\u4ecb\u7ecd\u548c\u4e2a\u4eba\u7406\u89e3</li> </ul>"},{"location":"#_7","title":"\ud83d\udcdd \u7b14\u8bb0\u6a21\u677f","text":"<p>\u6bcf\u7bc7\u8bba\u6587\u7b14\u8bb0\u901a\u5e38\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9\uff1a</p> <ul> <li>\u8bba\u6587\u57fa\u672c\u4fe1\u606f\uff08\u6807\u9898\u3001\u4f5c\u8005\u3001\u4f1a\u8bae/\u671f\u520a\u3001\u5e74\u4efd\uff09</li> <li>\u6458\u8981\uff1a\u8bba\u6587\u8981\u89e3\u51b3\u7684\u95ee\u9898\u548c\u4e3b\u8981\u8d21\u732e</li> <li>\u6838\u5fc3\u601d\u60f3\uff1a\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u70b9</li> <li>\u65b9\u6cd5\uff1a\u6280\u672f\u7ec6\u8282\u548c\u5b9e\u73b0\u65b9\u5f0f</li> <li>\u5b9e\u9a8c\u7ed3\u679c\uff1a\u4e3b\u8981\u5b9e\u9a8c\u7ed3\u8bba</li> <li>\u4e2a\u4eba\u7b14\u8bb0\uff1a\u4e2a\u4eba\u601d\u8003\u3001\u7591\u95ee\u548c\u5ef6\u4f38\u9605\u8bfb</li> </ul>"},{"location":"#_8","title":"\ud83d\ude80 \u66f4\u65b0\u65e5\u5fd7","text":"<p>\u672c\u7f51\u7ad9\u4f1a\u6301\u7eed\u66f4\u65b0\uff0c\u8bb0\u5f55\u6700\u65b0\u9605\u8bfb\u7684\u8bba\u6587\u3002\u6b22\u8fce\u8bbf\u95ee GitHub \u4ed3\u5e93 \u67e5\u770b\u66f4\u65b0\u5386\u53f2\u3002</p> <p>\u6700\u540e\u66f4\u65b0\uff1a2024\u5e74</p>"},{"location":"about/","title":"\u5173\u4e8e\u672c\u7ad9","text":""},{"location":"about/#_2","title":"\ud83c\udf93 \u5173\u4e8e\u4f5c\u8005","text":"<p>\u6211\u662f\u4e00\u540d\u5bf9\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u5145\u6ee1\u70ed\u60c5\u7684\u7814\u7a76\u8005/\u5b66\u4e60\u8005\u3002\u8fd9\u4e2a\u7f51\u7ad9\u662f\u6211\u6574\u7406\u8bba\u6587\u9605\u8bfb\u7b14\u8bb0\u3001\u6c89\u6dc0\u77e5\u8bc6\u7684\u4e2a\u4eba\u7a7a\u95f4\u3002</p>"},{"location":"about/#_3","title":"\ud83d\udca1 \u521b\u5efa\u521d\u8877","text":"<p>\u5728\u9605\u8bfb\u5927\u91cf\u8bba\u6587\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6211\u53d1\u73b0\uff1a</p> <ul> <li>\u8bba\u6587\u6570\u91cf\u591a\uff0c\u5bb9\u6613\u9057\u5fd8\u4e4b\u524d\u8bfb\u8fc7\u7684\u5185\u5bb9</li> <li>\u7f3a\u5c11\u7cfb\u7edf\u5316\u7684\u6574\u7406\uff0c\u96be\u4ee5\u5efa\u7acb\u77e5\u8bc6\u4f53\u7cfb</li> <li>\u597d\u7684\u60f3\u6cd5\u548c\u542f\u53d1\u9700\u8981\u8bb0\u5f55\u4e0b\u6765</li> </ul> <p>\u56e0\u6b64\u521b\u5efa\u4e86\u8fd9\u4e2a\u7f51\u7ad9\uff0c\u5e0c\u671b\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u7b14\u8bb0\u8bb0\u5f55\uff1a</p> <ol> <li>\u52a0\u6df1\u7406\u89e3\uff1a\u901a\u8fc7\u6574\u7406\u7b14\u8bb0\u5f3a\u5316\u5bf9\u8bba\u6587\u7684\u7406\u89e3</li> <li>\u77e5\u8bc6\u6c89\u6dc0\uff1a\u6784\u5efa\u4e2a\u4eba\u7684\u77e5\u8bc6\u5e93\uff0c\u65b9\u4fbf\u65e5\u540e\u67e5\u9605</li> <li>\u601d\u7ef4\u78b0\u649e\uff1a\u8bb0\u5f55\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u7684\u601d\u8003\u548c\u542f\u53d1</li> </ol>"},{"location":"about/#_4","title":"\ud83d\udee0\ufe0f \u6280\u672f\u6808","text":"<p>\u672c\u7ad9\u70b9\u4f7f\u7528\u4ee5\u4e0b\u6280\u672f\u6784\u5efa\uff1a</p> <ul> <li>MkDocs\uff1a\u9759\u6001\u7ad9\u70b9\u751f\u6210\u5668</li> <li>Material for MkDocs\uff1a\u73b0\u4ee3\u5316\u7684\u4e3b\u9898</li> <li>GitHub Pages\uff1a\u514d\u8d39\u6258\u7ba1</li> <li>GitHub Actions\uff1a\u81ea\u52a8\u5316\u90e8\u7f72</li> </ul>"},{"location":"about/#_5","title":"\ud83d\udcd6 \u7b14\u8bb0\u89c4\u8303","text":"<p>\u4e3a\u4e86\u4fdd\u6301\u7b14\u8bb0\u7684\u4e00\u81f4\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u6211\u9075\u5faa\u4ee5\u4e0b\u89c4\u8303\uff1a</p> <ul> <li>\u5ba2\u89c2\u8bb0\u5f55\uff1a\u51c6\u786e\u8bb0\u5f55\u8bba\u6587\u5185\u5bb9\uff0c\u4e0d\u6b6a\u66f2\u539f\u610f</li> <li>\u7ed3\u6784\u6e05\u6670\uff1a\u4f7f\u7528\u7edf\u4e00\u7684\u6a21\u677f\u7ec4\u7ec7\u5185\u5bb9</li> <li>\u91cd\u70b9\u7a81\u51fa\uff1a\u6807\u6ce8\u6838\u5fc3\u8d21\u732e\u548c\u5173\u952e\u521b\u65b0</li> <li>\u4e2a\u4eba\u601d\u8003\uff1a\u533a\u5206\u8bba\u6587\u5185\u5bb9\u548c\u4e2a\u4eba\u7406\u89e3</li> </ul>"},{"location":"about/#_6","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>GitHub \u4ed3\u5e93</li> </ul>"},{"location":"about/#_7","title":"\ud83d\udcc4 \u8bb8\u53ef\u534f\u8bae","text":"<p>\u672c\u7ad9\u70b9\u7684\u6240\u6709\u539f\u521b\u5185\u5bb9\u91c7\u7528 CC BY-NC-SA 4.0 \u8bb8\u53ef\u534f\u8bae\u3002</p> <p>\u8bba\u6587\u7248\u6743\u5f52\u539f\u4f5c\u8005\u6240\u6709\uff0c\u672c\u7ad9\u4ec5\u63d0\u4f9b\u5b66\u4e60\u7b14\u8bb0\u548c\u4e2a\u4eba\u7406\u89e3\u3002</p> <p>\u5982\u6709\u4efb\u4f55\u95ee\u9898\u6216\u5efa\u8bae\uff0c\u6b22\u8fce\u901a\u8fc7 GitHub Issues \u8054\u7cfb\u6211\uff01</p>"},{"location":"autotuning/","title":"Autotuning","text":"<p>Papers on automatic performance tuning, compiler optimization, and machine learning-guided optimization for tensor programs and GPU kernels.</p>"},{"location":"autotuning/#research-areas","title":"Research Areas","text":"<ul> <li>Tensor program optimization</li> <li>Cost models and performance prediction</li> <li>Search algorithms for program optimization</li> <li>GPU kernel optimization</li> <li>Compiler autotuning</li> </ul>"},{"location":"autotuning/#papers","title":"Papers","text":""},{"location":"autotuning/#tensor-program-optimization","title":"Tensor Program Optimization","text":"<ul> <li>Ansor - Automatic tensor program generation for deep learning (OSDI 2020, TVM)</li> <li>Learning to Optimize Tensor Programs - AutoTVM framework (NeurIPS 2018)</li> </ul>"},{"location":"autotuning/#cost-models-performance-prediction","title":"Cost Models &amp; Performance Prediction","text":"<ul> <li>MetaTune - Meta-learning based cost model for fast autotuning (2021)</li> <li>Learned Performance Model for TPU - ML-based TPU performance modeling (Google, 2020)</li> </ul>"},{"location":"autotuning/#gpu-kernel-optimization-portability","title":"GPU Kernel Optimization &amp; Portability","text":"<ul> <li>GPU Performance Portability - Autotuning for cross-vendor GPU portability (2025)</li> <li>Anatomy of Triton Attention Kernel - Building high-performance attention kernels with Triton (2025)</li> </ul> <p>Continuously updated...</p>"},{"location":"autotuning/anatomy-triton-attention/","title":"The Anatomy of a Triton Attention Kernel","text":"<p>Authors: Research Team Institution: Multiple Institutions Conference: arXiv 2025 Paper Link: arXiv:2511.11581</p> <p>Triton Attention Mechanism GPU Kernels</p>"},{"location":"autotuning/anatomy-triton-attention/#abstract","title":"Abstract","text":"<p>This paper develops a state-of-the-art paged attention kernel using exclusively OpenAI's Triton language, achieving state-of-the-art performance on both NVIDIA and AMD GPUs. It brings Triton attention kernel performance from 19.7% to 105.9% of state-of-the-art through systematic optimizations.</p> <p>Key Contributions: - Feature-complete cross-platform paged attention kernel - High-level approach with algorithmic and system-level improvements - Parameter auto-tuning for optimal performance - Open-sourced kernels adopted as default in vLLM for AMD GPUs</p>"},{"location":"autotuning/anatomy-triton-attention/#core-ideas","title":"Core Ideas","text":"<p>The paper demonstrates how to build production-quality attention kernels using high-level Triton code through systematic optimization. Triton enables writing GPU kernels in Python while abstracting away CUDA threading complexities like memory coalescing and tensor core scheduling. The resulting kernel achieves competitive or better performance than vendor-optimized implementations while maintaining cross-platform portability.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"autotuning/ansor/","title":"Ansor: Generating High-Performance Tensor Programs for Deep Learning","text":"<p>Authors: Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, Joseph E. Gonzalez, Ion Stoica Institution: UC Berkeley, University of Washington, AWS Conference: OSDI 2020 Paper Link: arXiv:2006.06762</p> <p>Tensor Programs Auto-Scheduling TVM</p>"},{"location":"autotuning/ansor/#abstract","title":"Abstract","text":"<p>Ansor is a tensor program generation framework for deep learning that automatically generates high-performance code without manual templates. It explores a large search space by sampling from a hierarchical representation and fine-tunes programs with evolutionary search and learned cost models.</p> <p>Key Contributions: - Template-free automatic tensor program generation - Hierarchical search space sampling with evolutionary search - Learned cost model for program performance prediction - Improves performance by up to 3.8\u00d7 (Intel CPU), 2.6\u00d7 (ARM CPU), 1.7\u00d7 (NVIDIA GPU)</p>"},{"location":"autotuning/ansor/#core-ideas","title":"Core Ideas","text":"<p>Unlike AutoTVM which requires manual templates, Ansor takes only tensor expressions as input and generates high-performance code automatically. It has three major components: a program sampler for diverse program generation, a performance tuner using evolutionary search, and a task scheduler for time resource allocation. Ansor is integrated into Apache TVM as the tvm.auto_scheduler package.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"autotuning/gpu-performance-portability/","title":"GPU Performance Portability needs Autotuning","text":"<p>Authors: B. Ringlein et al. Institution: Multiple Institutions Conference: arXiv 2025 Paper Link: arXiv:2505.03780</p> <p>GPU Portability Autotuning LLM Inference</p>"},{"location":"autotuning/gpu-performance-portability/#abstract","title":"Abstract","text":"<p>This paper makes the case for combining just-in-time (JIT) compilation with comprehensive kernel parameter autotuning to enable portable LLM inference with state-of-the-art performance without code changes, addressing vendor lock-in and hardware portability challenges.</p> <p>Key Contributions: - Explores up to 15\u00d7 more kernel parameter configurations - Produces significantly more diverse code across multiple dimensions - Outperforms vendor-optimized implementations by up to 230% - Reduces kernel code size by 70\u00d7 while eliminating manual optimizations</p>"},{"location":"autotuning/gpu-performance-portability/#core-ideas","title":"Core Ideas","text":"<p>As LLMs grow in complexity, reliance on a single dominant platform limits portability and creates vendor lock-in. Combining JIT compilation with comprehensive autotuning enables performance-portable LLM inference across different GPU vendors. The \"dejavu\" mechanism for Triton's autotuner reduces overhead to zero, enabling production use with full autotuning benefits.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"autotuning/learned-performance-model-tpu/","title":"A Learned Performance Model for Tensor Processing Units","text":"<p>Authors: Google Research Team Institution: Google Conference: arXiv 2020 Paper Link: arXiv:2008.01040</p> <p>TPU Performance Modeling Machine Learning</p>"},{"location":"autotuning/learned-performance-model-tpu/#abstract","title":"Abstract","text":"<p>This paper demonstrates a method of learning performance models from a corpus of tensor computation graph programs for Tensor Processing Unit (TPU) instances. The learned model outperforms heavily-optimized analytical performance models on tile-size selection and operator fusion tasks.</p> <p>Key Contributions: - Machine learning-based performance modeling for TPUs - Outperforms analytical models on key optimization tasks - Enables autotuning in settings where TPU access is limited or expensive - Reduces development burden for modeling contemporary accelerators</p>"},{"location":"autotuning/learned-performance-model-tpu/#core-ideas","title":"Core Ideas","text":"<p>Accurate hardware performance models are critical for efficient code generation but difficult to develop for complex processors. This research shows that learning from execution data can produce more accurate models than traditional analytical approaches, particularly useful for the proliferation of deep learning accelerators where analytical modeling is costly.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"autotuning/learning-optimize-tensor-programs/","title":"Learning to Optimize Tensor Programs","text":"<p>Authors: Tianqi Chen, Lianmin Zheng, Eddie Q. Yan, Ziheng Jiang, Thierry Moreau, et al. Institution: University of Washington, AWS, Cornell Conference: NeurIPS 2018 Paper Link: arXiv:1805.08166</p> <p>AutoTVM Machine Learning Compiler Optimization</p>"},{"location":"autotuning/learning-optimize-tensor-programs/#abstract","title":"Abstract","text":"<p>This paper introduces AutoTVM, a learning-based framework to optimize tensor programs for deep learning workloads. It learns domain-specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants.</p> <p>Key Contributions: - Machine learning-based tensor program optimization framework - Statistical cost models for guiding program search - Transfer learning acceleration (2-10\u00d7 speedup) - End-to-end performance improvements of 1.2\u00d7 to 3.8\u00d7</p>"},{"location":"autotuning/learning-optimize-tensor-programs/#core-ideas","title":"Core Ideas","text":"<p>AutoTVM addresses the limitation of manually optimized libraries like cuDNN that only support narrow ranges of hardware. It generates programs competitive with hardware-specific libraries while enabling operator fusion optimizations impossible with fixed operator libraries. The framework formalizes the optimization problem and proposes an ML solution that generalizes across different hardware targets.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"autotuning/metatune/","title":"MetaTune: Meta-Learning Based Cost Model for Fast and Efficient Auto-tuning Frameworks","text":"<p>Authors: Jaehun Ryu et al. Institution: Multiple Institutions Conference: arXiv 2021 Paper Link: arXiv:2102.04199</p> <p>Meta-Learning Cost Model TVM</p>"},{"location":"autotuning/metatune/#abstract","title":"Abstract","text":"<p>MetaTune is a meta-learning based cost model that quickly and accurately predicts the performance of optimized codes with pre-trained model parameters. It addresses the large space exploration and cost model training overheads in auto-tuning frameworks.</p> <p>Key Contributions: - GNN-based meta-learning for cost model training - Encodes convolution kernels as structurally similar graphs - 8-13% better inference time on average for four CNN models - Outperforms transfer learning by 10% in cross-platform cases</p>"},{"location":"autotuning/metatune/#core-ideas","title":"Core Ideas","text":"<p>MetaTune encodes convolution kernel codes as graphs to facilitate meta-learning, meta-trains a GNN model with minimal input data, and predicts optimization parameters for unseen operations during compilation. Implemented as an alternative cost model for TVM's auto-tuning framework, it provides comparable or lower optimization time while improving inference performance.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"machine-learning/","title":"\u673a\u5668\u5b66\u4e60","text":"<p>\u672c\u5206\u7c7b\u6536\u5f55\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u7ecf\u5178\u8bba\u6587\u548c\u524d\u6cbf\u7814\u7a76\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff1a</p> <ul> <li>\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u7406\u8bba</li> <li>\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1</li> <li>\u4f18\u5316\u7b97\u6cd5\u4e0e\u8bad\u7ec3\u6280\u5de7</li> <li>\u8868\u793a\u5b66\u4e60\u4e0e\u7279\u5f81\u63d0\u53d6</li> <li>\u8fc1\u79fb\u5b66\u4e60\u4e0e\u5143\u5b66\u4e60</li> <li>\u5f3a\u5316\u5b66\u4e60</li> </ul>"},{"location":"machine-learning/#_2","title":"\ud83d\udcd1 \u8bba\u6587\u5217\u8868","text":""},{"location":"machine-learning/#transformer","title":"\u6ce8\u610f\u529b\u673a\u5236\u4e0eTransformer","text":"<ul> <li>Attention Is All You Need - Google, NeurIPS 2017</li> <li>\u63d0\u51fa\u4e86\u5b8c\u5168\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684Transformer\u67b6\u6784\uff0c\u5f00\u542f\u4e86NLP\u9886\u57df\u7684\u65b0\u7eaa\u5143</li> </ul> <p>\u6301\u7eed\u66f4\u65b0\u4e2d...</p>"},{"location":"machine-learning/attention-is-all-you-need/","title":"Attention Is All You Need","text":"<p>\u4f5c\u8005: Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. \u673a\u6784: Google Brain, Google Research \u4f1a\u8bae: NeurIPS 2017 \u8bba\u6587\u94fe\u63a5: arXiv:1706.03762</p> <p>Transformer \u6ce8\u610f\u529b\u673a\u5236 \u5e8f\u5217\u5efa\u6a21</p>"},{"location":"machine-learning/attention-is-all-you-need/#_1","title":"\ud83d\udcdd \u6458\u8981","text":"<p>\u672c\u6587\u63d0\u51fa\u4e86 Transformer \u67b6\u6784\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b8c\u5168\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e8f\u5217\u8f6c\u6362\u6a21\u578b\uff0c\u6452\u5f03\u4e86\u5faa\u73af\u548c\u5377\u79ef\u7ed3\u6784\u3002\u5728\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e0a\uff0cTransformer \u4e0d\u4ec5\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u800c\u4e14\u8bad\u7ec3\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002</p> <p>\u4e3b\u8981\u8d21\u732e\uff1a - \u63d0\u51fa\u4e86\u7eaf\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u65e0\u9700\u5faa\u73af\u6216\u5377\u79ef - \u5f15\u5165\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08Multi-Head Self-Attention\uff09\u673a\u5236 - \u5728 WMT 2014 \u82f1\u5fb7\u548c\u82f1\u6cd5\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u8fbe\u5230 SOTA</p>"},{"location":"machine-learning/attention-is-all-you-need/#_2","title":"\ud83d\udca1 \u6838\u5fc3\u601d\u60f3","text":""},{"location":"machine-learning/attention-is-all-you-need/#1-self-attention","title":"1. \u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08Self-Attention\uff09","text":"<p>\u901a\u8fc7\u8ba1\u7b97\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u4f4d\u7f6e\u4e0e\u5176\u4ed6\u6240\u6709\u4f4d\u7f6e\u7684\u5173\u8054\u5ea6\uff0c\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff1a</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] <p>\u5176\u4e2d \\(Q\\)\uff08Query\uff09\u3001\\(K\\)\uff08Key\uff09\u3001\\(V\\)\uff08Value\uff09\u662f\u8f93\u5165\u7684\u7ebf\u6027\u53d8\u6362\u3002</p>"},{"location":"machine-learning/attention-is-all-you-need/#2-multi-head-attention","title":"2. \u591a\u5934\u6ce8\u610f\u529b\uff08Multi-Head Attention\uff09","text":"<p>\u5e76\u884c\u4f7f\u7528\u591a\u4e2a\u6ce8\u610f\u529b\u5934\uff0c\u6bcf\u4e2a\u5934\u5b66\u4e60\u4e0d\u540c\u7684\u8868\u793a\u5b50\u7a7a\u95f4\uff1a</p> \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O \\]"},{"location":"machine-learning/attention-is-all-you-need/#3-positional-encoding","title":"3. \u4f4d\u7f6e\u7f16\u7801\uff08Positional Encoding\uff09","text":"<p>\u7531\u4e8e\u6a21\u578b\u6ca1\u6709\u5faa\u73af\u7ed3\u6784\uff0c\u9700\u8981\u989d\u5916\u6dfb\u52a0\u4f4d\u7f6e\u4fe1\u606f\uff1a</p> \\[ \\begin{align} PE_{(pos, 2i)} &amp;= \\sin(pos / 10000^{2i/d_{model}}) \\\\ PE_{(pos, 2i+1)} &amp;= \\cos(pos / 10000^{2i/d_{model}}) \\end{align} \\]"},{"location":"machine-learning/attention-is-all-you-need/#_3","title":"\ud83c\udfd7\ufe0f \u6a21\u578b\u67b6\u6784","text":"<p>Transformer \u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff1a</p> <p>\u7f16\u7801\u5668\uff1a - 6 \u5c42\u5806\u53e0 - \u6bcf\u5c42\u5305\u542b\uff1a\u591a\u5934\u81ea\u6ce8\u610f\u529b + \u524d\u9988\u7f51\u7edc - \u6b8b\u5dee\u8fde\u63a5 + \u5c42\u5f52\u4e00\u5316</p> <p>\u89e3\u7801\u5668\uff1a - 6 \u5c42\u5806\u53e0 - \u6bcf\u5c42\u5305\u542b\uff1a\u63a9\u7801\u591a\u5934\u81ea\u6ce8\u610f\u529b + \u7f16\u7801\u5668-\u89e3\u7801\u5668\u6ce8\u610f\u529b + \u524d\u9988\u7f51\u7edc - \u6b8b\u5dee\u8fde\u63a5 + \u5c42\u5f52\u4e00\u5316</p>"},{"location":"machine-learning/attention-is-all-you-need/#_4","title":"\ud83d\udcca \u5b9e\u9a8c\u7ed3\u679c","text":""},{"location":"machine-learning/attention-is-all-you-need/#_5","title":"\u673a\u5668\u7ffb\u8bd1\u6027\u80fd","text":"\u6a21\u578b WMT'14 EN-DE WMT'14 EN-FR Transformer (big) 28.4 BLEU 41.8 BLEU ConvS2S 25.2 BLEU 40.5 BLEU"},{"location":"machine-learning/attention-is-all-you-need/#_6","title":"\u8bad\u7ec3\u6548\u7387","text":"<ul> <li>\u8bad\u7ec3\u65f6\u95f4\uff1a\u5728 8 \u4e2a P100 GPU \u4e0a\u8bad\u7ec3 3.5 \u5929\uff08base \u6a21\u578b\uff09</li> <li>\u5e76\u884c\u5316\uff1a\u76f8\u6bd4 RNN \u53ef\u4ee5\u66f4\u597d\u5730\u5e76\u884c\u5316\uff0c\u8bad\u7ec3\u901f\u5ea6\u5feb\u6570\u500d</li> </ul>"},{"location":"machine-learning/attention-is-all-you-need/#_7","title":"\ud83e\udd14 \u4e2a\u4eba\u7b14\u8bb0","text":""},{"location":"machine-learning/attention-is-all-you-need/#_8","title":"\u4f18\u52bf","text":"<ol> <li>\u5e76\u884c\u5316\u80fd\u529b\u5f3a\uff1a\u4e0d\u50cf RNN \u9700\u8981\u987a\u5e8f\u5904\u7406\uff0c\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97\u6240\u6709\u4f4d\u7f6e</li> <li>\u957f\u8ddd\u79bb\u4f9d\u8d56\uff1a\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u76f4\u63a5\u5efa\u6a21\u4efb\u610f\u8ddd\u79bb\u7684\u4f9d\u8d56\u5173\u7cfb</li> <li>\u53ef\u89e3\u91ca\u6027\uff1a\u6ce8\u610f\u529b\u6743\u91cd\u53ef\u4ee5\u53ef\u89c6\u5316\uff0c\u4e86\u89e3\u6a21\u578b\u5173\u6ce8\u54ea\u4e9b\u90e8\u5206</li> </ol>"},{"location":"machine-learning/attention-is-all-you-need/#_9","title":"\u5c40\u9650\u6027","text":"<ol> <li>\u8ba1\u7b97\u590d\u6742\u5ea6\uff1a\u81ea\u6ce8\u610f\u529b\u7684\u590d\u6742\u5ea6\u662f \\(O(n^2)\\)\uff0c\u5e8f\u5217\u8d8a\u957f\u8ba1\u7b97\u91cf\u8d8a\u5927</li> <li>\u4f4d\u7f6e\u7f16\u7801\uff1a\u56fa\u5b9a\u7684\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u9009\u62e9</li> <li>\u5c0f\u6570\u636e\u96c6\uff1a\u5728\u6570\u636e\u91cf\u8f83\u5c0f\u65f6\uff0cTransformer \u53ef\u80fd\u4e0d\u5982 RNN</li> </ol>"},{"location":"machine-learning/attention-is-all-you-need/#_10","title":"\u5f71\u54cd\u4e0e\u542f\u53d1","text":"<ul> <li>NLP \u9769\u547d\uff1aTransformer \u6210\u4e3a BERT\u3001GPT \u7b49\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u57fa\u7840</li> <li>\u8de8\u9886\u57df\u5e94\u7528\uff1aVision Transformer (ViT) \u5c06\u5176\u6210\u529f\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9</li> <li>\u7814\u7a76\u65b9\u5411\uff1a\u5982\u4f55\u964d\u4f4e\u81ea\u6ce8\u610f\u529b\u7684\u590d\u6742\u5ea6\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff08Linformer\u3001Performer \u7b49\uff09</li> </ul>"},{"location":"machine-learning/attention-is-all-you-need/#_11","title":"\ud83d\udd17 \u76f8\u5173\u8bba\u6587","text":"<ul> <li>BERT: Pre-training of Deep Bidirectional Transformers - \u57fa\u4e8e Transformer \u7684\u9884\u8bad\u7ec3\u6a21\u578b</li> <li>GPT Series - \u751f\u6210\u5f0f\u9884\u8bad\u7ec3 Transformer</li> <li>Vision Transformer (ViT) - Transformer \u5728\u89c6\u89c9\u9886\u57df\u7684\u5e94\u7528</li> </ul>"},{"location":"machine-learning/attention-is-all-you-need/#_12","title":"\ud83d\udccc \u5173\u952e\u4ee3\u7801\u7247\u6bb5","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n\n        # Linear projections and reshape\n        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n        # Scaled dot-product attention\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attention = torch.softmax(scores, dim=-1)\n        output = torch.matmul(attention, V)\n\n        # Concatenate heads and apply final linear\n        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        return self.W_o(output)\n</code></pre> <p>\u9605\u8bfb\u65e5\u671f\uff1a2024\u5e74 \u7b14\u8bb0\u72b6\u6001\uff1a\u5df2\u5b8c\u6210</p>"},{"location":"nlp/","title":"\u81ea\u7136\u8bed\u8a00\u5904\u7406","text":"<p>\u672c\u5206\u7c7b\u6536\u5f55\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u7ecf\u5178\u8bba\u6587\u548c\u524d\u6cbf\u7814\u7a76\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff1a</p> <ul> <li>\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b</li> <li>\u673a\u5668\u7ffb\u8bd1</li> <li>\u6587\u672c\u751f\u6210</li> <li>\u95ee\u7b54\u7cfb\u7edf</li> <li>\u4fe1\u606f\u62bd\u53d6</li> <li>\u5bf9\u8bdd\u7cfb\u7edf</li> </ul>"},{"location":"nlp/#_2","title":"\ud83d\udcd1 \u8bba\u6587\u5217\u8868","text":""},{"location":"nlp/#_3","title":"\u9884\u8bad\u7ec3\u6a21\u578b","text":"<ul> <li>BERT: Pre-training of Deep Bidirectional Transformers - Google AI, NAACL 2019</li> <li>\u63d0\u51fa\u53cc\u5411\u9884\u8bad\u7ec3 Transformer \u6a21\u578b\uff0c\u5237\u65b0\u591a\u9879 NLP \u4efb\u52a1\u8bb0\u5f55</li> </ul> <p>\u6301\u7eed\u66f4\u65b0\u4e2d...</p>"},{"location":"nlp/bert/","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","text":"<p>\u4f5c\u8005: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova \u673a\u6784: Google AI Language \u4f1a\u8bae: NAACL 2019 \u8bba\u6587\u94fe\u63a5: arXiv:1810.04805</p> <p>BERT \u9884\u8bad\u7ec3 \u53cc\u5411Transformer \u8bed\u8a00\u6a21\u578b</p>"},{"location":"nlp/bert/#_1","title":"\ud83d\udcdd \u6458\u8981","text":"<p>BERT\uff08Bidirectional Encoder Representations from Transformers\uff09\u662f\u4e00\u4e2a\u57fa\u4e8e Transformer \u7684\u53cc\u5411\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u5728\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u6587\u672c\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5fae\u8c03\uff0cBERT \u5728 11 \u4e2a NLP \u4efb\u52a1\u4e0a\u5237\u65b0\u4e86 SOTA \u8bb0\u5f55\u3002</p> <p>\u4e3b\u8981\u8d21\u732e\uff1a - \u63d0\u51fa\u63a9\u7801\u8bed\u8a00\u6a21\u578b\uff08MLM\uff09\u5b9e\u73b0\u771f\u6b63\u7684\u53cc\u5411\u9884\u8bad\u7ec3 - \u5f15\u5165\u4e0b\u4e00\u53e5\u9884\u6d4b\uff08NSP\uff09\u4efb\u52a1\u5b66\u4e60\u53e5\u5b50\u5173\u7cfb - \u5728\u591a\u4e2a NLP \u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347</p>"},{"location":"nlp/bert/#_2","title":"\ud83d\udca1 \u6838\u5fc3\u601d\u60f3","text":""},{"location":"nlp/bert/#1","title":"1. \u53cc\u5411\u4e0a\u4e0b\u6587\u5efa\u6a21","text":"<p>\u4e0e\u4f20\u7edf\u7684\u5355\u5411\u8bed\u8a00\u6a21\u578b\uff08\u5982 GPT\uff09\u4e0d\u540c\uff0cBERT \u901a\u8fc7\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u540c\u65f6\u5229\u7528\u5de6\u53f3\u4e0a\u4e0b\u6587\uff1a</p> <pre><code>\u4f20\u7edf LM:  The cat [sat] on the mat\n           \u2190\u2190\u2190\u2190\u2190\nGPT:      The cat [sat] on the mat\n           \u2192\u2192\u2192\u2192\u2192\nBERT:     The cat [MASK] on the mat\n           \u2190\u2190\u2190\u2192\u2192\u2192\u2192\n</code></pre>"},{"location":"nlp/bert/#2-masked-language-model","title":"2. \u63a9\u7801\u8bed\u8a00\u6a21\u578b\uff08Masked Language Model\uff09","text":"<p>\u968f\u673a\u63a9\u76d6\u8f93\u5165\u4e2d 15% \u7684\u8bcd\uff0c\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u88ab\u63a9\u76d6\u7684\u8bcd\uff1a</p> <ul> <li>80% \u66ff\u6362\u4e3a <code>[MASK]</code></li> <li>10% \u66ff\u6362\u4e3a\u968f\u673a\u8bcd</li> <li>10% \u4fdd\u6301\u4e0d\u53d8</li> </ul> <p>\u76ee\u6807\uff1a\u6700\u5927\u5316\u88ab\u63a9\u76d6\u8bcd\u7684\u6761\u4ef6\u6982\u7387</p>"},{"location":"nlp/bert/#3-next-sentence-prediction","title":"3. \u4e0b\u4e00\u53e5\u9884\u6d4b\uff08Next Sentence Prediction\uff09","text":"<p>\u7ed9\u5b9a\u53e5\u5b50\u5bf9 (A, B)\uff0c\u9884\u6d4b B \u662f\u5426\u662f A \u7684\u4e0b\u4e00\u53e5\uff1a</p> <pre><code>Input:  [CLS] Sentence A [SEP] Sentence B [SEP]\nLabel:  IsNext / NotNext\n</code></pre>"},{"location":"nlp/bert/#_3","title":"\ud83c\udfd7\ufe0f \u6a21\u578b\u67b6\u6784","text":"<p>BERT \u57fa\u4e8e Transformer \u7f16\u7801\u5668\uff0c\u6709\u4e24\u4e2a\u7248\u672c\uff1a</p> \u6a21\u578b \u5c42\u6570 \u9690\u85cf\u7ef4\u5ea6 \u6ce8\u610f\u529b\u5934 \u53c2\u6570\u91cf BERT-Base 12 768 12 110M BERT-Large 24 1024 16 340M"},{"location":"nlp/bert/#_4","title":"\u8f93\u5165\u8868\u793a","text":"<p>\u4e09\u79cd\u5d4c\u5165\u6c42\u548c\uff1a</p> <pre><code>Token Embeddings:    [CLS] my dog is cute [SEP] he likes play ##ing [SEP]\nSegment Embeddings:  E_A   E_A E_A E_A E_A E_A  E_B  E_B   E_B  E_B   E_B\nPosition Embeddings: E_0   E_1 E_2 E_3 E_4 E_5  E_6  E_7   E_8  E_9   E_10\n</code></pre>"},{"location":"nlp/bert/#_5","title":"\u7279\u6b8a\u6807\u8bb0","text":"<ul> <li><code>[CLS]</code>\uff1a\u5e8f\u5217\u5206\u7c7b\u6807\u8bb0\uff0c\u5176\u8f93\u51fa\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1</li> <li><code>[SEP]</code>\uff1a\u53e5\u5b50\u5206\u9694\u7b26</li> <li><code>[MASK]</code>\uff1a\u63a9\u7801\u6807\u8bb0</li> </ul>"},{"location":"nlp/bert/#_6","title":"\ud83d\udcca \u5b9e\u9a8c\u7ed3\u679c","text":""},{"location":"nlp/bert/#glue","title":"GLUE \u57fa\u51c6\u6d4b\u8bd5","text":"\u4efb\u52a1 \u6307\u6807 BERT-Base BERT-Large \u4e4b\u524d SOTA MNLI Acc 84.6/83.4 86.7/85.9 80.5/80.1 QQP F1 71.2 72.1 66.1 QNLI Acc 90.5 92.7 87.4 SST-2 Acc 93.5 94.9 93.2 CoLA Matthews Corr 52.1 60.5 35.0"},{"location":"nlp/bert/#squad","title":"SQuAD \u95ee\u7b54","text":"\u6a21\u578b SQuAD 1.1 F1 SQuAD 2.0 F1 BERT-Base 88.5 76.3 BERT-Large 93.2 83.1 \u4eba\u7c7b\u8868\u73b0 91.2 86.8 <p>BERT-Large \u5728 SQuAD 1.1 \u4e0a\u8d85\u8d8a\u4eba\u7c7b\u8868\u73b0\uff01</p>"},{"location":"nlp/bert/#conll-2003-ner","title":"\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08CoNLL-2003 NER\uff09","text":"\u6a21\u578b F1 \u4e4b\u524d SOTA 92.6 BERT-Large 92.8"},{"location":"nlp/bert/#_7","title":"\ud83e\udd14 \u4e2a\u4eba\u7b14\u8bb0","text":""},{"location":"nlp/bert/#_8","title":"\u5173\u952e\u8bbe\u8ba1","text":"<ol> <li>WordPiece \u5206\u8bcd\uff1a\u5904\u7406\u672a\u767b\u5f55\u8bcd\uff0c\u51cf\u5c0f\u8bcd\u8868\u5927\u5c0f</li> <li>Segment Embeddings\uff1a\u533a\u5206\u53e5\u5b50\u5bf9\u4e2d\u7684\u4e0d\u540c\u53e5\u5b50</li> <li>\u9884\u8bad\u7ec3 + \u5fae\u8c03\u8303\u5f0f\uff1a\u901a\u7528\u9884\u8bad\u7ec3 \u2192 \u4efb\u52a1\u7279\u5b9a\u5fae\u8c03</li> </ol>"},{"location":"nlp/bert/#_9","title":"\u4e3a\u4ec0\u4e48\u6709\u6548\uff1f","text":"<ul> <li>\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff1a\u4ece BooksCorpus (800M \u8bcd) + Wikipedia (2,500M \u8bcd) \u5b66\u4e60\u901a\u7528\u8bed\u8a00\u8868\u793a</li> <li>\u53cc\u5411\u5efa\u6a21\uff1a\u6bd4\u5355\u5411\u6a21\u578b\u80fd\u6355\u83b7\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587</li> <li>\u6df1\u5ea6 Transformer\uff1a\u5f3a\u5927\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b</li> </ul>"},{"location":"nlp/bert/#_10","title":"\u5c40\u9650\u6027","text":"<ol> <li>\u8ba1\u7b97\u6210\u672c\u9ad8\uff1a\u9884\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff084-16 TPU\uff0c\u51e0\u5929\u65f6\u95f4\uff09</li> <li>NSP \u4efb\u52a1\u4e89\u8bae\uff1a\u540e\u7eed\u7814\u7a76\uff08RoBERTa\uff09\u8868\u660e NSP \u53ef\u80fd\u4e0d\u5fc5\u8981</li> <li>[MASK] \u4e0d\u4e00\u81f4\uff1a\u9884\u8bad\u7ec3\u7528 [MASK]\uff0c\u4f46\u5fae\u8c03\u65f6\u6ca1\u6709</li> <li>\u6700\u5927\u957f\u5ea6\u9650\u5236\uff1a512 tokens \u9650\u5236\u4e86\u957f\u6587\u672c\u5904\u7406</li> </ol>"},{"location":"nlp/bert/#_11","title":"\u540e\u7eed\u53d1\u5c55","text":"<ul> <li>RoBERTa\uff1a\u79fb\u9664 NSP\uff0c\u52a8\u6001\u63a9\u7801\uff0c\u66f4\u5927\u6279\u6b21</li> <li>ALBERT\uff1a\u53c2\u6570\u5171\u4eab\uff0c\u53e5\u5b50\u987a\u5e8f\u9884\u6d4b\uff08SOP\uff09</li> <li>ELECTRA\uff1a\u5224\u522b\u5f0f\u9884\u8bad\u7ec3\uff0c\u66f4\u9ad8\u6548</li> <li>T5\uff1a\u7edf\u4e00 text-to-text \u6846\u67b6</li> <li>GPT-3/ChatGPT\uff1a\u5927\u89c4\u6a21\u751f\u6210\u5f0f\u9884\u8bad\u7ec3</li> </ul>"},{"location":"nlp/bert/#_12","title":"\u5e94\u7528\u6280\u5de7","text":"<p>\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\uff1a - \u5206\u7c7b\u4efb\u52a1\uff1a\u4f7f\u7528 [CLS] \u8f93\u51fa + \u7ebf\u6027\u5c42 - \u5e8f\u5217\u6807\u6ce8\uff1a\u6bcf\u4e2a token \u8f93\u51fa + \u7ebf\u6027\u5c42 - \u95ee\u7b54\u4efb\u52a1\uff1a\u9884\u6d4b\u7b54\u6848\u8d77\u59cb\u548c\u7ed3\u675f\u4f4d\u7f6e</p> <p>\u8d85\u53c2\u6570\uff1a - Batch size: 16, 32 - Learning rate: 5e-5, 3e-5, 2e-5 - Epochs: 2-4</p>"},{"location":"nlp/bert/#_13","title":"\ud83d\udd17 \u76f8\u5173\u8bba\u6587","text":"<ul> <li>Attention Is All You Need - Transformer \u67b6\u6784\u57fa\u7840</li> <li>ELMo: Deep contextualized word representations - \u65e9\u671f\u7684\u53cc\u5411\u9884\u8bad\u7ec3</li> <li>GPT: Improving Language Understanding by Generative Pre-Training - \u5355\u5411\u9884\u8bad\u7ec3</li> <li>RoBERTa: A Robustly Optimized BERT Pretraining Approach - BERT \u6539\u8fdb</li> <li>ELECTRA: Pre-training Text Encoders as Discriminators - \u66f4\u9ad8\u6548\u7684\u9884\u8bad\u7ec3</li> </ul>"},{"location":"nlp/bert/#_14","title":"\ud83d\udccc \u5173\u952e\u4ee3\u7801\u7247\u6bb5","text":"<pre><code>from transformers import BertTokenizer, BertForSequenceClassification\nimport torch\n\n# \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5206\u8bcd\u5668\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# \u51c6\u5907\u8f93\u5165\ntext = \"BERT is a powerful pre-trained model.\"\ninputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n\n# \u524d\u5411\u4f20\u64ad\noutputs = model(**inputs)\nlogits = outputs.logits\npredictions = torch.argmax(logits, dim=-1)\n\nprint(f\"Predictions: {predictions}\")\n</code></pre>"},{"location":"nlp/bert/#_15","title":"\u5fae\u8c03\u793a\u4f8b","text":"<pre><code>from transformers import BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import load_dataset\n\n# \u52a0\u8f7d\u6570\u636e\u96c6\ndataset = load_dataset('glue', 'sst2')\n\n# \u51c6\u5907\u6a21\u578b\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# \u8bad\u7ec3\u914d\u7f6e\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    learning_rate=2e-5,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n)\n\n# \u8bad\u7ec3\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['validation'],\n)\n\ntrainer.train()\n</code></pre> <p>\u9605\u8bfb\u65e5\u671f\uff1a2024\u5e74 \u7b14\u8bb0\u72b6\u6001\uff1a\u5df2\u5b8c\u6210</p>"},{"location":"simulation/","title":"Simulation","text":"<p>Papers on performance modeling, simulation frameworks, and analytical models for machine learning systems.</p>"},{"location":"simulation/#research-areas","title":"Research Areas","text":"<ul> <li>Hardware simulation for ML workloads</li> <li>Performance modeling and prediction</li> <li>Analytical models for inference/training</li> <li>Trace-based simulation</li> <li>Co-design and architecture exploration</li> </ul>"},{"location":"simulation/#papers","title":"Papers","text":""},{"location":"simulation/#performance-modeling-prediction","title":"Performance Modeling &amp; Prediction","text":"<ul> <li>AMALI - Analytical model for LLM inference on modern GPUs (ISCA 2025)</li> <li>NeuSight - Forecasting GPU performance for deep learning training and inference (ASPLOS 2025)</li> <li>LLMCompass - Hardware evaluation framework for LLM inference</li> <li>Forecasting LLM Inference - Hardware-agnostic analytical modeling (LIFE framework)</li> <li>Universal Performance - Universal performance modeling for ML training on multi-GPU platforms</li> </ul>"},{"location":"simulation/#llm-inference-serving-simulation","title":"LLM Inference &amp; Serving Simulation","text":"<ul> <li>VIDUR - Large-scale simulation framework for LLM inference (MLSys 2024, Microsoft)</li> <li>Frontier - Simulating next-generation LLM inference systems with MoE and disaggregation</li> <li>APEX - Extensible and dynamism-aware simulator for LLM serving</li> </ul>"},{"location":"simulation/#training-simulation-optimization","title":"Training Simulation &amp; Optimization","text":"<ul> <li>vTrain - Simulation framework for cost-effective LLM training (MICRO 2024)</li> <li>Astra-Sim - Distributed training simulator (Intel, Meta, Georgia Tech)</li> <li>Astra-Sim 2.0 - Enhanced version with hierarchical networks and disaggregated systems</li> </ul>"},{"location":"simulation/#benchmarking-tracing","title":"Benchmarking &amp; Tracing","text":"<ul> <li>Chakra - Execution trace format for ML workloads (Meta)</li> <li>Mystique - Production AI benchmark generation (ISCA 2023)</li> </ul>"},{"location":"simulation/#application-models","title":"Application Models","text":"<ul> <li>DLRM - Deep learning recommendation model (Facebook, 2019)</li> <li>Linformer - Self-attention with linear complexity</li> </ul>"},{"location":"simulation/#ai-powered-simulation","title":"AI-Powered Simulation","text":"<ul> <li>SimAI - Ansys AI-powered simulation platform for engineering</li> </ul> <p>Continuously updated...</p>"},{"location":"simulation/amali/","title":"AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs","text":"<p>Authors: Shiheng Cao, Junmin Wu, Junshi Chen, Hong An, Zhibin Yu Institution: Multiple Institutions Conference: ISCA 2025 Paper Link: ACM DL</p> <p>Performance Modeling LLM Inference GPU Architecture</p>"},{"location":"simulation/amali/#abstract","title":"Abstract","text":"<p>AMALI addresses the challenge of accurately modeling LLM inference workloads on modern GPUs. The model provides detailed performance prediction for LLM inference by incorporating specialized models for tensor cores, constant cache, and instruction cache.</p> <p>Key Contributions: - Instruction modifier and throughput-based tensor core model that captures math pipe throttle stalls - Analytical models for constant cache and instruction cache developed using micro-benchmarks - Multi-warp model leveraging warp instruction number distribution for LLM inference characteristics</p>"},{"location":"simulation/amali/#core-ideas","title":"Core Ideas","text":"<p>AMALI reduces mean absolute percentage error (MAPE) from 127.56% to 23.59% compared to the state-of-the-art GCoM model when validated on an A100 GPU. The framework can also be used for architecture design space exploration, accurately predicting end-to-end performance improvements with enhanced tensor core capability on H100.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/apex/","title":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving","text":"<p>Authors: Various Authors Institution: Multiple Institutions Conference: arXiv 2024 Paper Link: arXiv:2411.17651</p> <p>LLM Serving Parallel Execution Performance Simulation</p>"},{"location":"simulation/apex/#abstract","title":"Abstract","text":"<p>APEX is an LLM serving system simulator that efficiently identifies optimal parallel execution plans by considering key factors such as memory usage and batching behavior. The simulator performs dynamism-aware simulation to model iteration-level batching and leverages LLMs' repetitive structure to reduce design space exploration.</p> <p>Key Contributions: - Dynamism-aware simulation for iteration-level batching in LLM serving - Efficient design space exploration leveraging LLM repetitive structure - Scales to trillion-scale models with optimal parallel execution planning</p>"},{"location":"simulation/apex/#core-ideas","title":"Core Ideas","text":"<p>APEX addresses the challenge of selecting optimal parallel execution plans by balancing computation, memory, and communication overhead. It efficiently handles varying parallelism techniques (data, pipeline, tensor) and workload characteristics to identify the best execution strategy.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/astra-sim-2/","title":"ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems for Large-model Training at Scale","text":"<p>Authors: Collaboration between Intel, Meta, and Georgia Tech Institution: Intel, Meta, Georgia Tech Conference: IEEE 2023 Paper Link: arXiv:2303.14006</p> <p>Distributed Training Hierarchical Networks Disaggregated Systems</p>"},{"location":"simulation/astra-sim-2/#abstract","title":"Abstract","text":"<p>ASTRA-sim2.0 extends the open-source ASTRA-sim infrastructure with enhanced capabilities to model state-of-the-art and emerging distributed training models and platforms. It addresses the complex software/hardware co-design stack necessary for large-scale model training.</p> <p>Key Contributions: - Enhanced modeling of hierarchical network topologies - Support for disaggregated system architectures - Improved scalability for large-model training simulation</p>"},{"location":"simulation/astra-sim-2/#core-ideas","title":"Core Ideas","text":"<p>ASTRA-sim2.0 extends the original framework to handle modern training paradigms including hierarchical networks and disaggregated memory/compute systems. This enables more accurate simulation of emerging training platforms needed for models with billions to trillions of parameters.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/astra-sim/","title":"ASTRA-sim: Enabling SW/HW Co-Design Exploration for Distributed DL Training Platforms","text":"<p>Authors: Collaboration between Intel, Meta, and Georgia Tech Institution: Intel, Meta, Georgia Tech Conference: IEEE/Multiple Conferences Paper Link: ASTRA-sim Website</p> <p>Distributed Training Network Simulation Hardware-Software Co-design</p>"},{"location":"simulation/astra-sim/#abstract","title":"Abstract","text":"<p>ASTRA-sim is a distributed machine learning system simulator that models the end-to-end software and hardware stack of modern AI systems, including workload scheduling, collective communication algorithms, and hardware architectures (compute/memory/network).</p> <p>Key Contributions: - End-to-end cycle-accurate distributed training simulator - Support for multiple compute models (roofline, SCALE-sim) and network models (analytical, Garnet, NS3) - Enables systematic study of bottlenecks for scaling training of large models</p>"},{"location":"simulation/astra-sim/#core-ideas","title":"Core Ideas","text":"<p>ASTRA-sim enables design-space exploration for running large DNN models over future training platforms. It supports simulation from simple analytical to detailed cycle-accurate modeling of large-scale training platforms, particularly relevant for models like GPT-3 or DLRM that need to scale across thousands of accelerator nodes.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/chakra/","title":"Chakra: Execution Traces for Benchmarking and Network Performance Optimization","text":"<p>Authors: Meta AI Research Institution: Meta (Facebook) Conference: Engineering at Meta 2023 Paper Link: Meta Engineering Blog</p> <p>Execution Traces Benchmarking Performance Analysis</p>"},{"location":"simulation/chakra/#abstract","title":"Abstract","text":"<p>Chakra introduces an open graph-based representation of AI/ML workload execution, laying the foundation for benchmarking and network performance optimization. It provides a standardized schema for performance modeling through execution traces.</p> <p>Key Contributions: - Graph-based representation of AI/ML workload execution - Standardized Chakra execution trace format - Captures compute, communication, and memory operations with timing and dependencies - Integration with MLCommons for industry-wide adoption</p>"},{"location":"simulation/chakra/#core-ideas","title":"Core Ideas","text":"<p>Chakra execution traces capture arbitrary distributed ML workloads using a directed acyclic graph (DAG) representation of compute, communication, and remote memory nodes. In collaboration with MLCommons, Meta has open-sourced tools to enable collection, analysis, generation, and adoption of Chakra traces by simulators, emulators, and replay tools.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/dlrm/","title":"DLRM: Deep Learning Recommendation Model for Personalization and Recommendation Systems","text":"<p>Authors: Maxim Naumov et al. Institution: Facebook AI Research Conference: arXiv 2019 Paper Link: arXiv:1906.00091</p> <p>Recommendation Systems Deep Learning Embedding Tables</p>"},{"location":"simulation/dlrm/#abstract","title":"Abstract","text":"<p>DLRM is a state-of-the-art deep learning recommendation model developed by Facebook that handles both dense and sparse features. It uses specialized parallelization schemes with model parallelism on embedding tables and data parallelism on fully-connected layers.</p> <p>Key Contributions: - Novel interaction layer mimicking factorization machines - Specialized parallelization scheme for large-scale recommendation - Open-source implementation in PyTorch and Caffe2</p>"},{"location":"simulation/dlrm/#core-ideas","title":"Core Ideas","text":"<p>DLRM specifically interacts embeddings in a structured way that significantly reduces model dimensionality by only considering cross-terms produced by dot-products between pairs of embeddings. The model has become part of the popular MLPerf Benchmark and is widely used in production recommendation systems.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/forecasting-llm-inference/","title":"Forecasting LLM Inference Performance via Hardware-Agnostic Analytical Modeling","text":"<p>Authors: Rajeev Patwari, Ashish Sirasao, Devleena Das Institution: AMD Conference: arXiv 2025 Paper Link: arXiv:2508.00904</p> <p>LLM Inference Performance Forecasting Hardware-Agnostic</p>"},{"location":"simulation/forecasting-llm-inference/#abstract","title":"Abstract","text":"<p>LIFE (LLM Inference Forecast Engine) is a lightweight analytical framework for modeling LLM inference workloads in a hardware and data-agnostic manner. It predicts TTFT, TPOT and TPS using only TOPS and bandwidth, enabling hardware-aware performance estimation without requiring benchmarking datasets.</p> <p>Key Contributions: - Analytical models of core LLM operators - Configurable analytical workload model supporting various datatypes and optimizations - Empirical analysis of prefill/decode phases under varying prompt/generation lengths - Support for quantization, KV cache compression, chunked prefill, and different attention mechanisms</p>"},{"location":"simulation/forecasting-llm-inference/#core-ideas","title":"Core Ideas","text":"<p>LIFE provides hardware-agnostic performance forecasting that has been verified with actual inference of Llama2-7B on AMD Ryzen CPU, NPU, iGPU and NVIDIA V100 GPU hardware. The framework enables rapid performance estimation across different hardware platforms without extensive benchmarking.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/frontier/","title":"Frontier: Simulating the Next Generation of LLM Inference Systems","text":"<p>Authors: Yicheng Feng, Xin Tan, Kin Hang Sew, Yimin Jiang, Yibo Zhu, Hong Xu Institution: Multiple Institutions Conference: arXiv 2025 Paper Link: arXiv:2508.03148</p> <p>LLM Inference MoE Models Disaggregated Systems</p>"},{"location":"simulation/frontier/#abstract","title":"Abstract","text":"<p>Frontier is a high-fidelity simulator designed for next-generation LLM inference systems. It handles the growing complexity of Mixture-of-Experts (MoE) models and disaggregated architectures that decouple components like prefill/decode or attention/FFN for heterogeneous scaling.</p> <p>Key Contributions: - Unified framework for both co-located and disaggregated systems - Native support for MoE inference with expert parallelism - Simulation of complex workflows like cross-cluster expert routing - Advanced pipelining strategies for latency hiding</p>"},{"location":"simulation/frontier/#core-ideas","title":"Core Ideas","text":"<p>Frontier follows event-driven and modular design principles with a central GlobalController and modular ClusterWorkers. It achieves predicted throughput within 19.0% to 23.2% relative error margin, providing accurate performance trends for system-of-systems nature of modern LLM serving.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/linear/","title":"Linformer: Self-Attention with Linear Complexity","text":"<p>Authors: Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma Institution: Facebook AI Conference: arXiv 2020 Paper Link: arXiv:2006.04768</p> <p>Transformer Linear Attention Complexity Reduction</p>"},{"location":"simulation/linear/#abstract","title":"Abstract","text":"<p>Linformer reduces the overall self-attention complexity from O(n\u00b2) to O(n) in both time and space by demonstrating that the self-attention mechanism can be approximated by a low-rank matrix, making it practical for long sequence processing.</p> <p>Key Contributions: - Low-rank approximation of self-attention reducing complexity to linear - Maintains competitive performance with standard Transformer - Enables processing of much longer sequences efficiently</p>"},{"location":"simulation/linear/#core-ideas","title":"Core Ideas","text":"<p>The standard self-attention mechanism has O(n\u00b2) complexity with respect to sequence length. Linformer approximates the attention matrix with a low-rank factorization, achieving O(n) complexity while maintaining model quality. This breakthrough enables Transformer models to scale to significantly longer sequences.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/llmcompass/","title":"LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference","text":"<p>Authors: Princeton University Research Team Institution: Princeton University Conference: Research Publication 2024 Paper Link: GitHub</p> <p>Hardware Design LLM Inference Design Space Exploration</p>"},{"location":"simulation/llmcompass/#abstract","title":"Abstract","text":"<p>LLMCompass is a hardware evaluation framework for LLM inference workloads that is fast, accurate, versatile, and able to describe and evaluate different hardware designs. It includes an automatic mapper to find performance-optimal mapping and scheduling.</p> <p>Key Contributions: - Average 10.9% error rate for operator latency estimation, 4.1% for LLM inference - Area-based cost model for architectural design space exploration - Can simulate 4-NVIDIA A100 GPU node running GPT-3 175B in 16 minutes - Identifies designs achieving 3.41x improvement in performance/cost vs A100</p>"},{"location":"simulation/llmcompass/#core-ideas","title":"Core Ideas","text":"<p>Implemented as a Python library, LLMCompass democratizes hardware design space exploration research with fully open-source tools. It enables architects to reason about design choices through combined performance and area modeling, making it suitable for exploring cost-effective hardware accelerators for LLM workloads.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/mystique/","title":"Mystique: Enabling Accurate and Scalable Generation of Production AI Benchmarks","text":"<p>Authors: Research Team Institution: Multiple Institutions Conference: ISCA 2023 Paper Link: ACM DL</p> <p>Benchmarking AI Workloads Performance Analysis</p>"},{"location":"simulation/mystique/#abstract","title":"Abstract","text":"<p>Mystique enables accurate and scalable generation of production AI benchmarks. It addresses the challenge of creating representative benchmarks that reflect real-world AI workload characteristics for performance evaluation.</p> <p>Key Contributions: - Scalable benchmark generation for production AI workloads - Accurate representation of real-world AI characteristics - Framework for systematic performance evaluation</p>"},{"location":"simulation/mystique/#core-ideas","title":"Core Ideas","text":"<p>Mystique provides a systematic approach to generating benchmarks that accurately represent production AI workloads, enabling more realistic performance evaluation of AI systems and hardware. This is crucial for bridging the gap between research benchmarks and real-world deployment scenarios.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/neusight/","title":"NeuSight: Forecasting GPU Performance for Deep Learning Training and Inference","text":"<p>Authors: Research Team Institution: Multiple Institutions Conference: ASPLOS 2025 Paper Link: arXiv:2407.13853</p> <p>GPU Performance Deep Learning Performance Forecasting</p>"},{"location":"simulation/neusight/#abstract","title":"Abstract","text":"<p>NeuSight is a forecasting framework that predicts the performance of diverse deep learning models for both training and inference on unseen GPUs without requiring actual execution on the target GPU. It achieves exceptional accuracy compared to previous approaches.</p> <p>Key Contributions: - Reduces latency prediction error from 121.4% and 30.8% to 2.3% for GPT-3 on H100 - Inference error: 9.7% (vs prior work 220.9%), Training error: 7.3% (vs prior work 725.8%) - Tile-based approach predicting device utilization per tile - Works for both single-GPU and multi-GPU distributed settings</p>"},{"location":"simulation/neusight/#core-ideas","title":"Core Ideas","text":"<p>Unlike prior work that directly predicts kernel latency using machine learning, NeuSight distills kernel execution into tiles, predicts utilization per tile, and determines latency based on GPU architecture performance bounds. This novel approach significantly improves accuracy across various workloads and modern GPUs.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/simai/","title":"Ansys SimAI: AI for Accelerated Simulation","text":"<p>Authors: Ansys Inc. Institution: Ansys Inc. Conference: Product Release 2024 Paper Link: Ansys SimAI</p> <p>AI-Powered Simulation Engineering Design Optimization</p>"},{"location":"simulation/simai/#abstract","title":"Abstract","text":"<p>SimAI is an artificial intelligence platform for simulation that enables training AI models directly with simulation results and geometry input, predicting performance for new designs in a continuous 3D physical field. It reduces the overall design cycle by 10-100X.</p> <p>Key Contributions: - Physics-agnostic machine learning for simulation - 10-100X speedup in design cycle with full-fidelity accuracy - Geometry-based input enabling broader design exploration - Covers all physics domains from fluid mechanics to electromagnetics</p>"},{"location":"simulation/simai/#core-ideas","title":"Core Ideas","text":"<p>SimAI uses physics-agnostic machine learning techniques to learn data-driven representations of governing equations from previous simulations. Typically requiring 30-100 simulation results for training, it can predict performance for new designs with 2 days of training, enabling rapid design space exploration across automotive, aerospace, and telecommunications applications.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/universal-performance/","title":"Towards Universal Performance Modeling for Machine Learning Training on Multi-GPU Platforms","text":"<p>Authors: Research Team Institution: Multiple Institutions Conference: arXiv 2024, IEEE Paper Link: arXiv:2404.12674</p> <p>Performance Modeling Multi-GPU Distributed Training</p>"},{"location":"simulation/universal-performance/#abstract","title":"Abstract","text":"<p>This paper addresses characterizing and predicting training performance of modern ML workloads on compute systems with distributed compute and communication across CPUs, GPUs, and network devices. The framework achieves highly accurate performance prediction without running workloads on target hardware.</p> <p>Key Contributions: - Geomean error of 5.21% for DLRM models on multi-GPU platforms - Generalizes to Transformer-based NLP models with 3.00% error - 85% success rate in selecting fastest embedding table sharding configuration - Handles complexity of synchronization, load balancing, and various communication topologies</p>"},{"location":"simulation/universal-performance/#core-ideas","title":"Core Ideas","text":"<p>The prediction pipeline accurately predicts per-iteration training time across random configurations and generalizes well to different ML workload types. It addresses challenges including CPU-GPU synchronization, data distribution variance, and diverse communication devices (NVLink, PCIe, network cards), enabling rapid configuration optimization without extensive hardware benchmarking.</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/vidur/","title":"VIDUR: A Large-Scale Simulation Framework for LLM Inference","text":"<p>Authors: Microsoft Research Team Institution: Microsoft Research Conference: MLSys 2024 Paper Link: arXiv:2405.05465</p> <p>LLM Inference Serving Simulation Performance Optimization</p>"},{"location":"simulation/vidur/#abstract","title":"Abstract","text":"<p>Vidur is a high-fidelity, extensible simulation framework for LLM inference performance that enables studying system performance and capacity planning without requiring expensive GPU access. It achieves less than 9% error in estimating inference latency across multiple models.</p> <p>Key Contributions: - High-fidelity simulation using experimental profiling and predictive modeling - Capacity planning and deployment configuration optimization - Testing new research ideas (scheduling algorithms, speculative decoding) - Validated on LLaMA2 7/70B, InternLM-20B, and Qwen-72B</p>"},{"location":"simulation/vidur/#core-ideas","title":"Core Ideas","text":"<p>Vidur combines experimental profiling with predictive modeling to evaluate end-to-end inference performance. The companion tool Vidur-Search automatically identifies cost-effective deployment configurations meeting performance constraints, finding optimal LLaMA2-70B configuration in 1 hour on CPU (vs 42K GPU hours costing $218K for deployment-based exploration).</p> <p>Reading date: 2024 Note status: Completed</p>"},{"location":"simulation/vtrain/","title":"vTrain: A Simulation Framework for Evaluating Cost-Effective and Compute-Optimal Large Language Model Training","text":"<p>Authors: Research Team Institution: KAIST and collaborators Conference: MICRO 2024 Paper Link: arXiv:2312.12391</p> <p>LLM Training Cost Optimization Training Simulation</p>"},{"location":"simulation/vtrain/#abstract","title":"Abstract","text":"<p>vTrain is a profiling-driven simulator providing a fast yet accurate software framework to determine efficient and cost-effective LLM training system configurations. It addresses the critical challenge of training large models cost-effectively by exploring the parallelization design space.</p> <p>Key Contributions: - Fast design space exploration (under 200 seconds for full exploration) - Identifies competitive training plans within minutes - Supports multi-tenant GPU cluster scheduling optimization - Determines compute-optimal model architectures given fixed compute budget</p>"},{"location":"simulation/vtrain/#core-ideas","title":"Core Ideas","text":"<p>vTrain addresses limitations of heuristic-based parallel training strategies that leave significant performance on the table, wasting millions of dollars in training costs. Through case studies, it demonstrates effectiveness in evaluating optimal parallelization strategies balancing training time and cost, efficient multi-tenant scheduling, and compute-optimal model architecture selection.</p> <p>Reading date: 2024 Note status: Completed</p>"}]}